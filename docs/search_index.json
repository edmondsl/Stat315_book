[["index.html", "STAT 315 Notes Preliminaries Getting Started With R", " STAT 315 Notes Colorado State University Preliminaries Getting Started With R Installing R R is the base version of a free, open-source statistical computing program. R Studio is a nice graphic user interface (GUI) that works in conjunction with R. We will be using R Studio this semester as it is easier to learn how to use and has many nice shortcuts that aren’t available in the standard R download. Note: R and R Studio can not be installed on Chromebooks or other tablets/mini-computers. If you only have a tablet or a mini-computer, you can use To begin using R Studio, you will need to install “R” first and then install “R Studio” on your computer. Step 1: Download R Visit https://www.r-project.org/ Click CRAN under Download Select any of the mirrors Click the appropriate link for your type of system (Mac, Windows, Linux) Download R on this next page. For Windows, this will say install R for the first time For Mac, this will be under Latest release and will be something like R-4.1.0.pkg – the numbers may differ depending on the most recent version Install R on your computer Step 2: Download R Studio Visit https://www.rstudio.com/products/rstudio/download/#download Click to download Install R Studio on your computer Step 3: Verify R Studio is working Open R Studio Let’s enter a small dataset and calculate the average to make sure everything is working correctly. In the console, type in the following dataset of five homework scores: # Assignment using = x = c(90,75,100,85,90) # Alternative assignment using &lt;- x &lt;- c(90,75,100,85,90) # More on the differences between = and &lt;- at: # https://www.r-bloggers.com/2014/01/difference-between-assignment-operators-in-r/ In the console, calculate the average of the five homework scores: mean(x) ## [1] 88 Did you find the average of the five homework scores was 88? If so, you should be set up correctly! "],["basic-concepts-of-statistics.html", "Chapter 1 Basic Concepts of Statistics 1.1 Populations, Samples, and Processes 1.2 Methods of Collecting a Sample 1.3 Types of Data 1.4 Summarizing Qualitative Data 1.5 Visualizing Quantitative Data 1.6 Summary Statistics for Quantitative Data 1.7 Notation for Population Parameters and Sample Statistics 1.8 Descriptive Statistics vs. Inferential Statistics 1.9 R Companion for Chapter 1", " Chapter 1 Basic Concepts of Statistics In this chapter, we’ll introduce some of the basic concepts and definitions that are fundamental to the study of Probability and Statistics. In particular, sampling and sample statistics will be examined. 1.1 Populations, Samples, and Processes Definition 1.1 A population is a well-defined complete collection of objects. Definition 1.2 A sample is a subset of the population. Definition 1.3 A population parameter is a numerical value that describes a characteristic of a population. Definition 1.4 A sample statistic is a numerical value that describes a characteristic of a sample. Example 1.1 A student is interested in the average grade point average (GPA) of CSU students. She collects the GPAs of eight friends and finds an average of 2.87. For this example, determine the population, sample, parameter, and statistic. \\(~\\) \\(~\\) \\(~\\) \\(~\\) \\(~\\) 1.2 Methods of Collecting a Sample Definition 1.5 Simple Random Sampling (SRS) is a basic sampling method used in statistics where every individual or element in a population has an equal chance of being selected. This method ensures that the sample is unbiased. Example 1.2 Suppose we want to collect a simple random sample of CSU student GPAs. How might we do this? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.6 Convenience Sampling is a sampling method where the sample is taken from a group of people that are easy to reach or readily available. This method relies on selecting individuals who are most conveniently accessible to the researcher, rather than using a random or systematic approach. Example 1.3 Suppose we want to collect a convenience sample of CSU student GPAs. How might we do this? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.7 Stratified Sampling is a probability sampling method where the population is divided into distinct subgroups or “strata” that share similar characteristics, and a random sample is then taken from each stratum. This approach ensures that each subgroup is adequately represented in the final sample, leading to more precise and reliable estimates. Example 1.4 Suppose we want to collect poll data for the presidential election for 250 voters. Further, suppose a recent poll found that 36% of voters are registered with the Democratic Party, 31% of voters are registered with the Republican Party, and 33% are not registered for either major party. How might we collect a stratified sample and what’s the advantage compared to SRS? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Note: While there are many other more complex techniques to sample, we’ll generally assume that any sample collected in this class is a simple random sample. Data can be either collected in an observational study or in an experiment. Definition 1.8 In an observational study, data is collected without manipulating the environment or influencing how the data naturally occur. Example 1.5 How might one collect data to investigate a connection between smoking and cancer? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.9 In an experiment, participants are assigned to different treatment groups to determine if the treatment causes changes in the outcome. A key component is the control group, where participants receive no treatment or a standard one, serving as a baseline for comparison. This setup helps establish causality by isolating the effect of the treatment from other variables. Example 1.6 Why might it be difficult to conduct a controlled experiment investigating smoking and cancer? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Example 1.7 Suppose we want to set up a controlled experiment to investigate the efficacy of a new blood pressure reduction drug. We begin by randomly sampling 100 people with high blood pressure. How would we set up a controlled experiment? \\(~\\) \\(~\\) \\(~\\) \\(~\\) In addition to the sampling method, bias may still occur if participants in an experiment are aware of the treatment they are receiving. This awareness can influence their behavior or responses, potentially skewing the results. Example 1.8 20 patients are being treated for depression, half are given an antidepressant and half are given a sugar pill (). Suppose the patients know which pill they have been given. How might this cause the results to be biased? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.10 Blinded Experiments A blinded experiment is an experiment in which participants do not know which treatment they are receiving. A double-blinded experiment is an experiment in which both the participants and the researchers do not know which treatment is being administered during the experiment. A triple-blinded experiment is an experiment in which the participants, the researchers, and the analysts evaluating the data are all unaware of which treatment is being administered. This approach further reduces the potential for bias at all stages of the experiment. 1.3 Types of Data We will break data into two categories: quantitative and qualitative. Definition 1.11 Quantitative data is numeric data or numbers and can broken into two further categories, discrete and continuous. Discrete data is quantitative data with a finite or countably infinite number of values. Continuous data is quantitative data with an uncountably infinite number of values or data taken from an interval. Example 1.9 Give examples of discrete and continuous data. \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.12 Qualitative data refers to names, categories, or descriptions and can also be broken down into two further categories, nominal and ordinal. Nominal data is qualitative data with no natural ordering. Ordinal data is qualitative data with a natural ordering. Example 1.10 Give examples of nominal and ordinal data. \\(~\\) \\(~\\) \\(~\\) \\(~\\) 1.4 Summarizing Qualitative Data While we will mostly focus on quantitative data in this class, there are a few simple methods for describing qualitative data that you should be aware of.\\ Example 1.11 Let’s collect some data on eye color from members of the class and then explore some statistical and graphical summaries. Data: Frequency Table Bar Graph Proportions: \\(p=\\frac{\\#\\text{ in category}}{\\#\\text{ total}}\\) What proportion are brown? Percentages: \\(P\\% = p \\cdot 100\\) What percentage are blue? 1.5 Visualizing Quantitative Data Definition 1.13 A histogram visualizes the distribution of a dataset by grouping data into bins and plotting the frequency of data points in each bin. The x-axis represents the data range, and the y-axis shows the frequency of occurrences. Histograms are useful for understanding the shape, central tendency, and spread of data. Example 1.12 14 students were randomly sampled and were asked how many credits they are taking this semester. Data: 12, 14, 15, 12, 9, 18, 22, 15, 16, 14, 14, 12, 3, 17 Class Frequency Relative Frequency Percentage Frequency \\([0,3)\\) \\([3,6)\\) \\([6,9)\\) \\([9,12)\\) \\([12,15)\\) \\([15,18)\\) \\([18,21)\\) \\([21,24)\\) Total Histogram ## Loading required package: gridExtra ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine Histogram shapes 1. Unimodal 2. Bimodal 3. Bell-shaped 4. Positively (right) skewed - tail is on the right 5. Negatively (left) skewed - tail is on the left 6. Symmetric Box Plots Definition 1.14 A box plot, or box-and-whisker plot, visualizes data distribution using a five-number summary: minimum, first quartile, median, third quartile, and maximum [more on these later]. The box shows the interquartile range (IQR), and the whiskers extend to values within 1.5 times the IQR. Outliers are plotted as individual points. Box plots are useful for comparing distributions and spotting outliers. Example 1.13 Here are some examples of box plots. 1.6 Summary Statistics for Quantitative Data Example 1.14 For the following exercises, we will use the following data set. The heights of five students are measured to be (in cm): 183, 165, 165, 175, 187. Notation: \\(\\sum_{i=1}^n x_i = x_1 + x_2 + ... + x_n\\) Example 1.15 What is \\(\\sum x_i\\), the sum of all heights in the sample? \\(~\\) \\(~\\) \\(~\\) \\(~\\) There are many ways to measure the ``center” of our data set. We will look at mean, median, and mode. Definition 1.15 The sample mean (or sample average) \\(\\bar{x}\\) is given by: \\(\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}\\). Example 1.16 What is the sample mean of the original heights data set, \\(\\bar{x}\\)? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.16 The sample median \\(\\tilde{x}\\) is obtained by first ordering the \\(n\\) observations from smallest to largest and then calculated by the following:\\ \\[ \\tilde{x} = \\begin{cases} \\mbox{the single middle value if $n$ is odd}, &amp; \\tilde{x} = x_{(n+1)/2} \\\\ \\mbox{the average of the two middle values if $n$ is even}, &amp; \\tilde{x} = ( x_{n/2} + x_{n/2 + 1 })/2 \\end{cases} \\] Example 1.17 What is the sample median of the heights data set, \\(\\tilde{x}\\)? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Example 1.18 If we measure an additional person at 174cm, what is the new median of the data set? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.17 The sample mode is the most commonly occurring data point. Example 1.19 What is the sample mode of the heights data set? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Measures of Variability There are a few different ways to describe the variability of a data set, including range, variance, standard deviation, and interquartile range. Definition 1.18 The range of a data set is the difference between the largest and the smallest values. Example 1.20 What is the range of the heights data set? \\(~\\) \\(~\\) \\(~\\) \\(~\\) Typically, the most common method for measuring the spread of a data set is variance or standard deviation. Definition 1.19 The sample variance, denoted by \\(s^2\\), is given by: \\(s^2 = \\sum_{i=1}^n\\frac{(x_i - \\bar{x})^2}{n-1}\\) Definition 1.20 The sample standard deviation, denoted by s, is given by: \\(s = \\sqrt{s^2} = \\sqrt{\\sum_{i=1}^n\\frac{(x_i - \\bar{x})^2}{n-1}}\\) Example 1.21 Using the heights data set, calculate the sample variance and standard deviation. i 1 2 3 4 5 \\(\\Sigma\\) \\(x_i\\) \\(x_i - \\bar{x}\\) \\((x_i - \\bar{x})^2\\) \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.21 The z-score measures the number of standard deviations an observation is from the mean: \\(z_i = \\frac{x_i-\\bar{x}}{s}\\) Definition 1.22 A percentile is a measure of relative standing. The \\(p^{th}\\) percentile is the number where at least p% of the data values are less than or equal to this number. Special percentiles: \\(25^{th}\\) percentile = 1\\(^{st}\\) quartile = \\(Q_1\\) \\(50^{th}\\) percentile = 2\\(^{nd}\\) quartile = \\(Q_2\\) = median \\(75^{th}\\) percentile = 3\\(^{rd}\\) quartile = \\(Q_3\\) Example 1.22 Calculate \\(Q_1\\) and \\(Q_3\\) of the heights data set. \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.23 The interquartile range (IQR) is a measure of statistical dispersion, representing the difference between the first quartile (Q1) and the third quartile (Q3). It captures the range of the middle 50% of the data, indicating the spread of the central portion of the dataset. It is given by: \\(Q_3 - Q_1\\). Example 1.23 Calculate IQR for the heights data set. \\(~\\) \\(~\\) \\(~\\) \\(~\\) Definition 1.24 An outlier is a data point that lies far outside of the normal range of the rest of the data. Rule of thumb: Data points smaller than \\(Q_1 - 1.5 \\cdot IQR\\) or larger than \\(Q_3 + 1.5 \\cdot IQR\\) are outliers. Definition 1.25 The five-number summary is a statistical description of a dataset, consisting of the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. It provides a quick overview of the data’s distribution, highlighting its center and spread. The five numbers are: minimum, \\(Q_1\\), median, \\(Q_3\\), maximum. Example 1.24 What is the five number summary for the heights data set? \\(~\\) \\(~\\) \\(~\\) \\(~\\) 1.7 Notation for Population Parameters and Sample Statistics Some care must be given when reporting a mean, variance, or standard deviation, as it is more important to distinguish between population and sample. Mean Median Variance Standard Deviation Population \\(\\mu\\) \\(\\tilde{\\mu}\\) \\(\\sigma^2\\) \\(\\sigma\\) Sample \\(\\bar{x}\\) \\(\\tilde{x}\\) \\(s^2\\) \\(s\\) 1.8 Descriptive Statistics vs. Inferential Statistics After a sample is taken, there are a variety of ways to describe the sample. This is the realm of Descriptive Statistics. We can describe our sample with numbers (statistics) or pictorially. Later in the semester, we will examine Inferential Statistics. In Inferential Statistics, our goal is to use characteristics of a sample to say something about the full population (see Inferential Statistics). Connection between Probability and Statistics If we know the characteristics of a population, the study of Probability will help us determine the likelihood that a sample will have certain characteristics. Likewise, Statistics can potentially help us use the characteristics of a sample to say something about the population. 1.9 R Companion for Chapter 1 Let’s manually enter the heights data set (183, 170, 160, 175, 187) from chapter 1 and the following R commands will do the calculations we completed by hand. Lines of code that begin with are comments and are not executed by the computer. Lines of code that begin with denote the output. # manually enter your data by using the &quot;c&quot; command heights = c(183,165,165,175,187) # calculate sample statistics on data set mean(heights) ## [1] 175 median(heights) ## [1] 175 var(heights) ## [1] 102 sd(heights) ## [1] 10.0995 We can also create a histogram in R. Note that we can specify the breaks for the bins. In this case, the breaks are defined at 160, 170, 180, 190. hist(heights,breaks=c(160,170,180,190)) Let’s create a randomized dataset of eye colors and analyze the data. When we generate random data in this class, we will use the function, so that we all generate the same random data. If we don’t use the command before generating random data, we will all get different random data. Here, we generate 47 random eye colors and take a look at the first few observations. set.seed(2020) eye.colors = sample(c(&quot;amber&quot;, &quot;blue&quot;, &quot;brown&quot;, &quot;gray&quot;, &quot;green&quot;, &quot;hazel&quot;, &quot;red&quot;), 47, replace = T) # the &quot;head&quot; function will give the first few data values # or the &quot;header&quot; of the dataset head(eye.colors) ## [1] &quot;gray&quot; &quot;gray&quot; &quot;red&quot; &quot;hazel&quot; &quot;amber&quot; &quot;amber&quot; We can create a frequency table of these values and create a barplot. table(eye.colors) ## eye.colors ## amber blue brown gray green hazel red ## 3 10 3 10 7 8 6 barplot(table(eye.colors), xlab = &quot;eye color&quot;, ylab = &quot;frequency&quot;, main = &quot;Bar plot of eye color occurance&quot;) We can also calculate proportion and percentage tables. table(eye.colors) # shows how many of each value exist ## eye.colors ## amber blue brown gray green hazel red ## 3 10 3 10 7 8 6 length(eye.colors) # tells how many data points there are ## [1] 47 table(eye.colors)/length(eye.colors) # proportion ## eye.colors ## amber blue brown gray green hazel red ## 0.06382979 0.21276596 0.06382979 0.21276596 0.14893617 0.17021277 0.12765957 table(eye.colors)/length(eye.colors)*100 # percent ## eye.colors ## amber blue brown gray green hazel red ## 6.382979 21.276596 6.382979 21.276596 14.893617 17.021277 12.765957 "],["probability.html", "Chapter 2 Probability 2.1 Sample Space and Events 2.2 Set Theory 2.3 Axioms and Rules of Probability 2.4 Counting Techniques 2.5 Conditional Probability 2.6 Independence 2.7 Objective vs. Subjective Probability 2.8 R Companion for Chapter 2", " Chapter 2 Probability In this chapter, we’ll delve into Probability, the branch of mathematics that deals with the study of randomness and uncertainty, helping us understand and quantify the likelihood of different outcomes. 2.1 Sample Space and Events Definition 2.1 An experiment is any activity or process whose outcome is subject to uncertainty. Definition 2.2 The sample space of an experiment, denoted by \\(\\Omega\\) or \\(\\mathcal{S}\\), is the set of all possible outcomes of that experiment. Definition 2.3 An event is any collection (subset) of outcomes contained in the sample space \\(\\Omega\\). Example 2.1 For the following examples, define the sample space and give an example of an event. Code for running the experiment in R is given. Experiment 1: Roll a fair six-sided die. # roll one fair six-sided die in R sample(x=1:6,size=1) ## [1] 5 Experiment 2: Roll two fair six-sided dice. # roll two fair six-sided dice in R sample(x=1:6,size=2) ## [1] 1 3 Experiment 3: Randomly select a real number in the closed interval from 0 to 1. # generate one random number between 0 and 1 runif(n=1) ## [1] 0.6137196 2.2 Set Theory Note: For the following examples, suppose a fair six-sided die is rolled, so \\(\\Omega=\\{1,2,3,4,5,6\\}\\). Define the following events: \\(A = \\text{roll an even} = \\{2,4,6\\}\\), \\(B=\\text{roll a 3 or higher} =\\{3,4,5,6\\}\\), \\(C=\\text{roll a prime number}=\\{2,3,5\\}\\). Definition 2.4 A complement of an event A, denoted by \\(A^c\\) or A’, is the set of all outcomes in \\(\\Omega\\) that are not contained in A. Illustration Definition 2.5 The union of two events A and B, denoted by \\(A \\cup B\\) and read “A or B”, is the event consisting of all outcomes that are either in A or B or in both. Illustration Definition 2.6 The intersection of two events A and B, denoted by \\(A \\cap B\\) and read “A and B”, is the event consisting of all outcomes that are in both A and B. Illustration Definition 2.7 The set difference of two events A and B, denoted by AB and read “difference of A and B”, is the event consisting of all outcomes that are in A but not in B. Illustration Definition 2.8 The null or empty set is the set of no events (or elements), denoted \\(\\emptyset\\). Definition 2.9 When \\(A \\cap B = \\emptyset\\), A and B are said to be mutually exclusive (or disjoint) events. Example 2.2 Suppose a fair six-sided die is rolled, so \\(\\Omega=\\{1,2,3,4,5,6\\}\\). Define the following events: \\(A = \\text{roll an even} = \\{2,4,6\\}\\), \\(B=\\text{roll a 3 or higher} =\\{3,4,5,6\\}\\), \\(C=\\text{roll a prime number}=\\{2,3,5\\}\\). Determine the following sets and fill in the Venn diagrams representing these sets. Example 2.3 Draw and determine \\(A \\cup B^c\\). Example 2.4 Draw and determine \\(A \\cap B \\cap C^C\\). Example 2.5 Draw and determine \\([A \\cup B] \\cap C\\). Example 2.6 Draw and determine \\(A \\cap B \\cap C\\). 2.3 Axioms and Rules of Probability Definition 2.10 The probability of an event, \\(A\\), is denoted by \\(P(A)\\) and represents the likelihood or chance that the event \\(A\\) will occur. Note that \\(0 \\leq P(A) \\leq 1\\). Axioms of Probability The axioms of probability were introduced by the mathematician Andrey Kolmogorov in 1933. These axioms form the foundation of probability theory, providing a rigorous, axiomatic framework for quantifying uncertainty. In mathematics, an axiomatic approach means that all rules and theorems are built from a set of fundamental principles, or axioms. Axiom 1: For any event A, \\(P(A) \\ge 0\\) Axiom 2: \\(P(\\Omega) = 1\\) Axiom 3: If \\(A_1, A_2, A_3, \\dots\\) is an infinite collection of disjoint events, then \\[P \\big( \\cup_{i=1}^{\\infty} A_i \\big) = P(A_1 \\cup A_2 \\cup A_3 \\cup \\dots) = \\sum_{i=1}^{\\infty} P(A_i)\\] Probability Rules The following can be derived from the Axioms of Probability. \\(P(\\emptyset) = 0\\) For any event A, \\(P(A) + P(A^c)\\) = 1 \\(P(A^c) = 1 - P(A)\\) For any event A, \\(0 \\le P(A) \\le 1\\) For any two events A and B, \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) For any three events A, B, and C, \\(P(A \\cup B \\cup C) = P(A) + P(B) + P(C) - P(A \\cap B) - P(A \\cap C) - P(B \\cap C) + P(A \\cap B \\cap C)\\). \\(\\left[A \\cup B\\right]^C = A^C \\cap B^C\\) \\(\\left[A \\cap B\\right]^C = A^C \\cup B^C\\) Example 2.7 Suppose our class has the following breakdown: 70% Math students 20% seniors 30% Born in Denver 10% Math students and seniors 15% Math students and born in Denver 10% Seniors and Born in Denver 5% Math students, seniors, and born in Denver Define events as follows: A = Math student, B = senior, C = Born in Denver. Suppose a random student from the class is selected. Calculate the following probabilities. \\(P(A \\cup B)\\) \\(P(C \\backslash B)\\) \\(P([A \\cap C]^c)\\) \\(P(A \\cup B \\cup C)\\) 2.4 Counting Techniques Equally Likely Outcomes If \\(\\Omega = \\lbrace x_1, x_2, ... x_N \\rbrace\\) and all outcomes are equally likely, then \\(P(A) = |A| / N\\), where \\(|A|\\) is the number of outcomes contained in the event A. Example 2.8 You roll a fair six-sided die. Calculate \\(P(even)\\). Theorem 2.1 (Product Rule for Ordered Pairs) If the first element of an ordered pair can be selected in \\(n_1\\) ways and for each of the these \\(n_1\\) ways the second element of the pair can be selected in \\(n_2\\) ways, then the number of pairs is \\(n_1 \\cdot n_2\\). Example 2.9 You flip three fair coins. Calculate the probability that all are heads or all are tails. Theorem 2.2 (Generalized Product Rule) Suppose a set consists of k elements (k-tuples) and that there are \\(n_1\\) possible choices for the first element, \\(n_2\\) possible choices for the second element, … , and \\(n_k\\) possible choices for the \\(k^{th}\\) element, then there are \\(n_1 \\cdot n_2 \\cdot \\cdots n_k\\) possible k-tuples. Example 2.10 How many license plates can be chosen if the first three symbols are letters and the last three symbols are numbers? The factorial function, \\(n!\\), is important in counting methods and probability. It is the product of all positive integers less than or equal to \\(n\\), that is, \\(n! =n \\cdot (n-1) \\cdot (n-2) \\cdot \\ldots \\cdot 2 \\cdot 1 \\text{ for } n \\in \\mathbb{Z}^+\\). Note: \\(0! = 1\\) Definition 2.11 An ordered subset is called a permutation. The number of permutations of size \\(k\\) that can be formed from the \\(n\\) elements in a set will be denoted by \\(P_{n,k}\\). \\(\\boxed{P_{n,k} = \\frac{n!}{(n-k)!}}\\) Definition 2.12 An unordered subset is called a combination. The number of combinations of size \\(k\\) that can be formed from the \\(n\\) elements in a set will be denoted by \\(\\binom{n}{k}\\) or \\(C_{n,k}\\). \\(\\boxed{\\binom{n}{k} = C_{n,k} = \\frac{n!}{k! \\cdot (n-k)!}}\\) Example 2.11 Suppose there are five competitors in an Olympic competition. A gold, a silver, and a bronze medal are awarded for the top three competitors. Are combinations or permutations more appropriate for this situation? How many possible ways can these three awards be given out? Example 2.12 Suppose there are five competitors in an Olympic competition and only three can advance to the final competition. Are combinations or permutations more appropriate for this situation? How many possible ways can you choose three people from a group of five? Example 2.13 Powerball is an American lottery game where participants pay $2 for a chance of winning the grand prize, typically hundreds of millions of dollars. Participants choose five numbers between 1 and 69 and one additional number between 1 and 26. If they choose all the numbers correctly, they win the grand prize. What is the probability of matching all six numbers? Example 2.14 Suppose you are dealt three cards from a standard 52-card playing deck. What is the probability that you get a heart, a diamond, and a club in that order? What if order doesn’t matter? 2.5 Conditional Probability Conditional probability is the likelihood of an event occurring given that another event has already occurred. It refines our understanding of probability by taking into account additional information that might affect the outcome. Understanding conditional probabilities is crucial in many areas of statistics and decision-making, as it allows us to update our predictions and assessments based on new evidence. Definition 2.13 For any two events A and B with \\(P(B)&gt;0\\), the is defined by: \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\] Example 2.15 The following is a contingency table from a survey on congressional approval. Political Affiliation Approves of Congress Disapproves of Congress Total Republican 100 20 120 Democrat 20 80 100 Independent 40 40 80 Total 160 140 300 What is the probability that a randomly selected individual from the survey approves of Congress? What is the probability that a randomly selected individual approves of Congress given that they are Democrat? Given that the randomly selected person approves of the Congress, what is the probability that they are Republican? Theorem 2.3 Multiplication Rule: \\(P(A \\cap B) = P(A|B) \\cdot P(B)\\) Example 2.16 Suppose that people with a genetic variation have a 50% chance of getting a rare disease. The probability of having the genetic variation is 1%. What is the probability that a randomly chosen person has the genetic variation and also has the disease? Theorem 2.4 Law of Total Probability: Let \\(A_1, A_2, \\ldots, A_k\\) be mutually exclusive and exhaustive events (i.e., the events form a partition). Then for any event B, \\[P(B) = P(B|A_1) \\cdot P(A_1) + P(B|A_2) \\cdot P(A_2) + \\ldots + P(B|A_k) \\cdot P(A_k) \\] Illustration of a Partition Example 2.17 Box #1 has ten computer chips (five working and five broken). Box #2 has five (one working and four broken). Suppose we randomly select one of the two boxes and then randomly select a chip. What is the probability that the chip is broken? Theorem 2.5 Bayes’ Theorem: Suppose that \\(A_1 \\cap A_2 = \\emptyset\\) and \\(A_1 \\cup A_2 = \\Omega\\), then if \\(P(B)&gt;0\\), \\[ P(A_1|B) = \\frac{P(B|A_1)P(A_1)}{P(B|A_1)P(A_1)+P(B|A_2)P(A_2)} \\] Let \\(A_1, A_2, \\ldots, A_k\\) be a collection of mutually exclusive and exhaustive events, then if \\(P(B)&gt;0\\), \\[P(A_j|B) = \\frac{P(A_j \\cap B)}{P(B)} = \\frac{P(B|A_j) \\cdot P(A_j)}{\\sum_{i=1}^k P(B|A_i) \\cdot P(A_i)} \\] Example 2.18 Suppose a test to detect a rare disease is positive 95% of the time for a person with a disease and is positive 1% of the time for a person without the disease. Further, assume that 0.5% of the population has the disease. Given that a person tests positive for a disease, what is the probability that they actually has the disease? 2.6 Independence Recall: Definition 2.14 When \\(A \\cap B = \\emptyset\\), A and B are said to be mutually exclusive (or disjoint) events. Definition 2.15 Two events A and B are independent if \\(P(A|B) = P(A)\\) and dependent otherwise. Also, A and B are independent if and only if \\(P(A \\cap B) = P(A) \\cdot P(B)\\). Example 2.19 Recall from the previous example that a test to detect a rare disease is positive 95% of the time for a person with a disease and is positive 1% of the time for a person without the disease. Further, assume that 0.5% of the population has the disease. Let \\(D\\) be the event that one has the disease and let \\(P\\) be the event that one has a positive test. Are D and P mutually exclusive? Are D and P independent? 2.7 Objective vs. Subjective Probability There are two different interpretations of Probability. Both follow the Axioms of Probability. Definition 2.16 Objective Probability refers to the probability of an event as determined by the long-term frequency of its occurrence, based on repeated trials or observations. It is rooted in empirical data and reflects the likelihood of an event as the proportion of times it occurs in a large number of trials. Example 2.20 I flip a coin 100 times and it lands on head 53 times. I estimate the probability of flipping a heads to be \\(P_{100}(head) = \\frac{53}{100} = 0.53\\). As we flip the coin more and more, we would expect that \\(P_n(head) \\to 0.5\\) as \\(n \\to \\infty\\) if the coin is actually a fair coin. Definition 2.17 Subjective Probability refers to the probability of an event as determined by an individual’s personal judgment or belief, rather than objective data. It reflects the degree of confidence one has in the occurrence of an event. A well-known approach to subjective probability is Bayesian probability, which updates these beliefs based on new evidence. Example 2.21 I am given a coin which I have no reason to believe it is unfair (i.e, \\(P(head) = 0.5\\)). I flip the coin 100 times and 3 land on heads. Since this is very unlikely to happen if the coin is fair, I update my estimate to be \\(P(head)=0.1\\) (i.e., somewhere in between \\(3/100=0.03\\) and \\(1/2 = 0.5\\)). 2.8 R Companion for Chapter 2 Example 2.22 Let’s simulate a fair six-sided die roll. Recall that we need to use the function so that we all generate the same random data. For consistency, I always set the seed to 2020, i.e., set.seed(2020). The sample function will generate 100 random die rolls with each possible outcome (1,2,3,4,5,6) being equally likely. set.seed(2020) data = sample(x = 1:6,size = 100,replace = T); data ## [1] 4 4 6 1 1 4 2 6 1 5 2 2 6 5 2 3 2 5 4 2 6 6 4 6 4 2 4 5 4 4 3 6 2 2 6 3 5 4 ## [39] 5 5 2 5 1 6 3 5 1 5 3 1 5 3 2 6 2 1 3 2 1 6 5 5 2 5 6 4 3 6 4 4 2 2 1 6 1 4 ## [77] 6 2 2 2 1 4 1 1 4 2 2 5 5 4 4 4 6 5 4 1 6 5 1 4 Let’s make a histogram of the simulated data. Recall that each outcome (1,2,3,4,5,6) is equally likely in theory but in our simulation, some numbers may come up more than others. To make the histogram look better, I define the boundary breaks to be 0:6 (i.e., 0,1,2,3,4,5,6). hist(data,breaks=0:6) With only 100 random die rolls, the distribution isn’t as flat, uniform, and symmetric as we would expect. The number 3 came up less than we would anticipate. This is OK. We can also generate a frequency table and use this to create a barplot. This looks a little nicer than the standard histogram and doesn’t require use to use the argument. table(data) ## data ## 1 2 3 4 5 6 ## 15 21 8 21 18 17 barplot(table(data)) If we want a better representation of the true distribution of the fair six-sided die rolls, we could simulate 10000 die rolls instead of 100. set.seed(2020) data = sample(x = 1:6,size = 10000,replace = T) table(data) ## data ## 1 2 3 4 5 6 ## 1630 1657 1733 1655 1629 1696 barplot(table(data)) Simulation can be helpful for us to verify hand calculations and to estimate quantities that are hard to calculate by hand. Our simulation results won’t exactly match our hand calculations, but if the sample size is large enough, they should be close.\\ For instance, we know that P(Roll=1) = 1/6 for the population of all fair six-sided die rolls. If you refer to the table at the bottom of the previous page, you see that 1 came up 1630 times out of 10000 die rolls. In the simulation, P(1) = 1630/10000 = 0.1630 which is pretty close the the theoretical value of P(1) = 1/6 = 0.1667.\\ Here is an example of how to calculate the probability of a die roll less than 3 in our simulation. # Calculate P(Roll &lt; 3) # From theory, we know that P(Roll &lt; 3) = P(1) + P(2) = 1/6 + 1/6 = 1/3 # count up the number of rolls less than 3 num.under3 = sum( data &lt; 3 ) # divide the number of rolls less than 3 by the number of rolls prob.under3 = num.under3 / 10000 prob.under3 ## [1] 0.3287 # alternatively: prob.under3 = mean(data &lt; 3) prob.under3 ## [1] 0.3287 From theory, we expect P(Roll \\(&lt;\\) 3) = 1/3 = 0.3333. In our simulation, we found P(Roll \\(&lt;\\) 3) = 0.3287. Example 2.23 Let’s simulate rolling two fair six-sided dice 10,000 times and calculate some probabilities. will represent the first die roll and will represent the second die roll. set.seed(2020) d1 = sample(x = 1:6,size = 10000,replace = T) d2 = sample(x = 1:6,size = 10000,replace = T) From our simulated data, let’s calculate P(D1 = 1 and D2 = 2), that is, the probability that first die roll is 1 and the second die roll is 2. From theory, this probability should be 1/36 = 0.0278. (We use the for in R.) num.events = sum(d1 == 1 &amp; d2 == 2) prob = num.events / 10000; prob ## [1] 0.0256 Next, let’s calculate P(D1 \\(&lt;\\) 2 or D2 \\(&gt;\\) 2), that is, the probability that the first die roll is less than 2 or the second die roll is greater than 2. From theory, this probability should be 0.7222. We use the for the in R.) num.events = sum(d1 &lt; 2 | d2 &gt; 2) prob = num.events / 10000; prob ## [1] 0.7183 Finally, let’s look at the frequency table and distribution of the sum of the two die rolls. table(d1+d2) ## ## 2 3 4 5 6 7 8 9 10 11 12 ## 271 540 824 1154 1373 1703 1360 1149 814 525 287 barplot(table(d1+d2),xlab=&quot;Sum of Dice&quot;,ylab=&quot;Counts&quot;) "],["discrete-random-variables.html", "Chapter 3 Discrete Random Variables 3.1 Random Variables 3.2 Probability Distributions for Discrete Random Variables 3.3 Expected Values (Means) 3.4 Bernoulli and Binomial Random Variables 3.5 Geometric, Discrete Uniform, and Poisson Random Variables 3.6 R Companion for Chapter 3", " Chapter 3 Discrete Random Variables In this chapter, we’ll explore random variables and their associated probability distributions, with a focus on discrete random variables. These concepts are the backbone of probability theory, enabling us to model and analyze uncertainty. We’ll also briefly cover the differences between discrete and continuous random variables—discrete variables take on specific, countable values, while continuous variables can assume any value within a range. 3.1 Random Variables Definition 3.1 For a given sample space \\(\\Omega\\) of an experiment, a random variable (RV) is a rule that assigns a number to each outcome in \\(\\Omega\\). Mathematically, a random variable is a function with the sample space as its domain and the set of real numbers as its range. Definition 3.2 A discrete random variable is a random variable whose possible values either constitute a finite set or a countably infinite set. Definition 3.3 A continuous random variable is a random variable whose possible values are uncountably infinite or defined over an interval. Note: \\(P(X=c)=0\\) for a continuous random variable \\(X\\) and a constant \\(c\\). Example 3.1 For the following cases, determine whether the random variable is discrete or continuous and state the sample space, \\(\\Omega\\). Roll a fair six-sided die. Let \\(X\\) be the number on the top of the die. Pick a random person and let \\(Y\\) be the number of keys in their pocket. Suppose a student is given 50 minutes to complete an exam. Let \\(Z\\) be the amount of time it takes the student to complete the exam. 3.2 Probability Distributions for Discrete Random Variables Definition 3.4 The probability mass function (pmf) or probability distribution of a discrete random variable \\(X\\) is defined for every number \\(x\\) by \\(p(x) = P(X=x)\\). Note: \\(\\sum_{x \\in X} p(x) = 1\\) Definition 3.5 The cumulative distribution function (CDF) \\(F(x)\\) of a discrete random variable \\(X\\) with pmf \\(p(x)\\) is defined for every number x by: \\(F(x) = P(X \\le x) = \\sum_{y:y \\le x} p(y)\\) \\(F(x)\\) gives you the probability that a random variable will be at most equal to \\(x\\). \\(F(x)\\) must be right-continuous and non-decreasing. \\(F(x)\\) must satisfy two limit laws: \\(\\lim_{x \\to -\\infty} F(x) = 0\\) \\(\\lim_{x \\to \\infty} F(x) = 1\\) Theorem 3.1 For any two numbers a and b with \\(a \\le b\\), \\(P(a \\le X \\le b ) = F(b) - F(a^-)\\). Example 3.2 Roll a fair six-sided die. Let \\(X\\) be the number of the top of the die. Find the pmf and cdf of \\(X\\) and sketch them. Example 3.3 Flip two fair coins. Let \\(Y\\) be the number of coins that land on heads. Find the pmf and cdf of \\(Y\\) and sketch both. 3.3 Expected Values (Means) Definition 3.6 Let X be a discrete random variable with set of possible outcomes \\(D\\) and pmf \\(p(x)\\). The expected value or mean value of \\(X\\), denoted \\(E[X]\\) or \\(\\mu\\), is: \\[E[X] = \\sum_{x \\in D} x \\cdot p(x)\\] Theorem 3.2 \\(E[h(X)] = \\sum p(x) \\cdot h(x)\\) Theorem 3.3 \\(E[aX+b] = a \\cdot E[X] + b\\) Definition 3.7 Let X have a pmf \\(p(x)\\) and expected value \\(\\mu\\). Then the variance of X, denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is: \\[Var(X) = \\sigma^2 = \\sum_{x \\in D} (x-\\mu)^2 \\cdot p(x) = E[(X-\\mu)^2]\\] Definition 3.8 The standard deviation, denoted SD or \\(\\sigma\\) is: \\[SD(X) = \\sigma = \\sqrt{Var(X)}\\] Theorem 3.4 \\(Var(X) = E[X^2] - (E[X])^2\\) Theorem 3.5 \\(Var(aX+b) = a^2 \\cdot \\sigma^2\\) Example 3.4 Roll a fair six-sided die. Let \\(X\\) be the number of the top of the die. Find \\(E[X]\\) and \\(Var(X)\\). \\(x\\) 1 2 3 4 5 6 \\(\\Sigma\\) \\(p(x)\\) \\(x*p(x)\\) \\(x^2*p(x)\\) Example 3.5 Flip two fair coins. Let \\(Y\\) be the number of coins that land on heads.\\ Find \\(E[Y]\\) and \\(Var(Y)\\). \\(x\\) 0 1 2 \\(\\Sigma\\) \\(p(x)\\) \\(x*p(x)\\) \\(x^2*p(x)\\) Example 3.6 Suppose \\(Z = 3 \\cdot Y + 2\\), where \\(Y\\) is defined above. Find \\(E[Z]\\) and \\(Var(Z)\\). 3.4 Bernoulli and Binomial Random Variables Definition 3.9 Suppose \\(p(x)\\) depends on a quantity that can be assigned any one of a number of possible values, with each different value determining a different probability distribution. Such a quantity is called a parameter of the distribution. Definition 3.10 The collection of all probability distributions for different values of the parameter is called a family of probability distributions. Definition 3.11 A Bernoulli(\\(p\\)) random variable \\(X\\) is a discrete random variable with two possible outcomes (typically, these outcomes are 0 and 1). The PMF of a Bernoulli(\\(p\\)) is: If \\(X \\sim \\text{Bernoulli(}p)\\)) random variable, then \\(E[X] = p\\) and \\(Var(X) = p(1-p)\\) Example 3.7 Show that if \\(X\\) is a Bernoulli(\\(p\\)) random variable, then \\(E[X] = p\\) and \\(Var(X) = p(1-p)\\). Definition 3.12 \\(X\\) is a Binomial(\\(n\\),\\(p\\)) random variable if \\(X\\) is a discrete random variable that satisfies the following conditions: The PMF of a Binomial(\\(n,p\\)) is: \\(p(x) = \\binom{n}{x} p^x (1-p)^{n-x}, x = 0, 1, 2, \\ldots, n\\) If \\(X \\sim \\text{Binomial(}n,p)\\) random variable, then \\(E[X] = np\\) and \\(Var(X) = np(1-p)\\). Example 3.8 Basketball player Lebron James has a career free throw percentage of 73.1% (i.e., there’s a 73.1% chance he will make a basket from the free throw line). Suppose Lebron has six free throw attempts in a game and assume all free throw shots are independent. Answer the following questions. On average, how many free throws is Lebron expected to make? What is the variance? Let \\(X\\) be the number of free throws that Lebron makes. Find and sketch the pmf of \\(X\\). \\(x\\) 0 1 2 3 4 5 6 \\(p(x)\\) What is the probability that Lebron makes at least five shots? 3.5 Geometric, Discrete Uniform, and Poisson Random Variables Definition 3.13 \\(X\\) is a Geometric(\\(p\\)) random variable is \\(X\\) is a discrete random variable with the following properties. The PMF of a Geometric(\\(p\\)) is: \\(p(x) = p(1-p)^{x-1}, x = 1, 2, 3, \\ldots\\) If \\(X \\sim \\text{Geometric(}p)\\) random variable, then \\(E[X] = \\frac{1}{p}\\) and \\(Var(X) = \\frac{1-p}{p^2}\\). Example 3.9 Basketball player Lebron James has a career free throw percentage of 73.1%. Suppose he takes shots until makes one. Calculate the probability that 3 or more shots will be required to make his first shot. Definition 3.14 \\(X\\) is a Discrete Uniform(a,b) random variable is \\(X\\) is a discrete random variable such that the outcomes \\(a, a+1, \\ldots, b\\) are equally likely. Let \\(n = b-a+1\\). The PMF of a Uniform(\\(a,b\\)) is: \\(p(x) = \\frac{1}{n}, x = a, a+1, \\ldots, b\\) If \\(X \\sim \\text{Uniform(}a,b)\\) random variable, then \\(E[X] = \\frac{a+b}{2}\\) and \\(Var(X) = \\frac{(b-a+1)^2-1}{12}\\). Example 3.10 Using the formulas above, calculate the expected value and variance of a fair six-sided die roll. Definition 3.15 A Poisson random variable is a discrete random variable with parameter \\(\\lambda\\) and the following pmf: \\[p(x)=\\frac{e^{-\\lambda} \\lambda^x}{x!}, x = 0, 1, 2, \\ldots \\] If \\(X \\sim \\text{Poisson(}\\lambda)\\) random variable, then \\(E[X] = \\lambda\\) and \\(Var(X) = \\lambda\\). Note: Poisson random variables are often used to model the number of events that occur in a finite period of time. Example 3.11 Jihan is going fishing and on average, she catches 2 fish per day. Assume that the time between successive fish caught is independent. What is the probability that Jihan catches less than 3 fish in a given day? 3.6 R Companion for Chapter 3 We can calculate probabilities for common discrete random variables easily in R. There are two parts to finding probabilities using the PMF or CDF. R uses four prefixes to reference difference elements of a random variable. These are: p for “probability”, the cumulative distribution function (CDF) q for “quantile”, the inverse CDF d for “density”, the probability mass function (PMF) r for “random”, a random variable having the specified distribution In addition, we have suffixes for common random variables: binom (binomial), pois (Poisson), and geom (Geometric). Note that geom is defined in terms of failures instead of total trials. For instance, suppose that \\(X \\sim Binomial(n=10,p=0.5)\\), that is, X is a binomial random variable that counts the number of heads in 10 fair coin flips. We can calculate P(X=5), the probability of exactly 5 heads in 10 fair coin flips, as follows. dbinom(x=5,size=10,prob=0.5) ## [1] 0.2460938 If we want the probability of at most five heads in 10 coin flips, \\(P(X \\leq 5)\\), we can use ***pbinom}. pbinom(q=5,size=10,prob=0.5) ## [1] 0.6230469 Now, suppose that \\(Y \\sim Poisson(\\lambda = 2)\\) and we want to know \\(P(Y &gt; 3)\\). First, we note that \\(P(Y &gt; 3) = 1 - P(Y \\leq 3)\\). 1-ppois(q=3,lambda=2) ## [1] 0.1428765 If we use the prefix r, we can generate simulated data according to a particular random variable. Let’s generate 10,000 observations from a Poisson distribution with \\(\\lambda=3\\), look at the first few values, and then calculate some summary statistics. This will require us to use the rpois function to generate random Poisson data. rand.data = rpois(n=10000,lambda=3) head(rand.data) # look at the first few values ## [1] 3 6 2 4 1 4 mean(rand.data) # calculate the mean ## [1] 3.0088 var(rand.data) # calculate the variance ## [1] 3.047827 From theory, we know that if \\(Y \\sim Poisson(\\lambda=3)\\), then \\(E[Y] = Var(Y) = \\lambda = 3\\). Our simulated results are pretty close to the theoretical values. table(rand.data) ## rand.data ## 0 1 2 3 4 5 6 7 8 9 10 11 ## 520 1458 2252 2213 1676 1031 514 211 79 28 15 3 barplot(table(rand.data)) How would you describe the distribution above? It seems to be asymmetric, skewed right (tail on the right), and unimodal. Example 3.12 Let’s simulate some data and calculate the probabilities in the sample. Suppose we flip 10 fair coins and we count the number of heads. Let X be the number of heads. We can model X as a binomial random variable with parameters n=10 (number of coin flips) and p=0.5 (fair coin). Let’s simulate running this experiment 100 times. set.seed(2020) coins = rbinom(n=100,size=10,prob=0.5) table(coins) ## coins ## 1 2 3 4 5 6 7 8 9 ## 3 1 12 19 32 19 8 5 1 barplot(table(coins)) In our simulation, what was the probability that 5 heads occurred out of 10 flips? In the above table, we can see we got 5 heads on 32 of the 100 simulations, so \\(P(5) = 32/100 = 0.32\\).\\ We could also calculate as follows: p5 = sum(coins == 5)/100; p5 ## [1] 0.32 What is the probability that we got more than 7 heads? p = sum(coins &gt; 7)/100; p ## [1] 0.06 "],["continuous-random-variables.html", "Chapter 4 Continuous Random Variables 4.1 Probability Density Function (pdf) 4.2 Cumulative Distribution Functions 4.3 Expected Values 4.4 Normal Distribution 4.5 The Exponential and Uniform Random Variables 4.6 (Continuous) Uniform[a,b] Random Variable 4.7 R Companion for Chapter 4", " Chapter 4 Continuous Random Variables In this chapter, we will explore continuous random variables, which are types of random variables that can take any value within an uncountable set, such as an interval or a combination of intervals. We will also delve into some of the most commonly encountered continuous random variables, with a particular focus on the Normal distribution. A continuous random variable is a random variable whose possible values either constitute an interval of real numbers or a union of intervals of real numbers. 4.1 Probability Density Function (pdf) Definition 4.1 Let \\(X\\) be a continuous random variable. The probability distribution or probability density function (pdf) of \\(X\\) is a function \\(f(x)\\) such that for any two numbers a and b with \\(a \\le b\\), \\(P(a \\le X \\le b) = \\int_a^b f(x) dx\\). Note: All valid pdfs must satisfy two conditions: (1) \\(f(x) \\geq 0\\) (\\(f(x)\\) is non-negative). (2) \\(\\int_{-\\infty}^\\infty f(x) dx = 1\\) (area under \\(f(x)\\) is equal to 1) Example 4.1 Let \\(X\\) be the IQ of a randomly chosen individual which follow a normal distribution with mean 100 and standard deviation 15. Example 4.2 Let \\(Y\\) be a random real number between 0 and 1. (Note: this can be done approximately in R using the function.) Example 4.3 Suppose \\(X\\) is a random variable with the following pdf: \\(f(x) = \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\frac{x}{2} &amp; 0 \\le x \\le 2 \\\\ 0 &amp; x &gt; 2\\\\ \\end{cases}\\) Is \\(f(x)\\) a valid pdf? What is \\(P(X &lt; 1)\\)? What is \\(P(1/2 \\le X \\le 3/2)\\)? What is \\(P(X=1)\\)? Caution! For a continuous random variable \\(X\\), \\(P(X=c)=0\\) for all constants \\(c\\). While it may be possible for \\(X\\) to be exactly equal to the constant \\(c\\), it can only happen with probability zero. In general, when dealing with continuous random variables, we consider the probability that \\(X\\) takes on a value over an interval rather than a specific value. 4.2 Cumulative Distribution Functions Definition 4.2 The cumulative distribution function (CDF) of a continuous random variable \\(X\\) is defined for every number \\(x\\) by \\(F(x) = P(X \\le x) = \\int_{-\\infty}^x f(y) dy\\). F(x) represents the area under the curve to the left of \\(x\\). Note: Conditions on \\(F(x)\\): \\(0 \\le F(x) \\le 1\\) \\(\\lim_{x \\to -\\infty} F(x) = 0\\) \\(\\lim_{x \\to \\infty} F(x) = 1\\) Theorem 4.1 If \\(X\\) is a continuous random variable with pdf \\(f(x)\\) and CDF \\(F(x)\\), then at every x at which the derivative \\(F&#39;(x)\\) exists, \\(F&#39;(x) = f(x)\\). Theorem 4.2 Let X be a continuous random variable with pdf \\(f(x)\\) and CDF \\(F(x)\\). Then we have the following: For any number \\(a\\), \\(P(X&gt;a) = 1-F(a)\\) For any two numbers \\(a\\) and \\(b\\) with \\(a \\le b\\), \\(P(a \\le X \\le b) = F(b) - F(a)\\). Example 4.4 Suppose \\(X\\) is a random variable with the following pdf: \\(f(x) = \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\frac{x}{2} &amp; 0 \\le x \\le 2 \\\\ 0 &amp; x &gt; 2\\\\ \\end{cases}\\) Find the CDF of \\(X\\) and sketch it. Use the CDF to calculate \\(P(X \\leq 1)\\). 4.3 Expected Values Definition 4.3 The expected value or mean of a continuous random variable \\(X\\) with pdf \\(f(x)\\) is: \\[ E[X] = \\mu = \\int_{-\\infty}^{\\infty} x \\cdot f(x) dx\\] Theorem 4.3 If \\(X\\) is a continuous random variable with pdf \\(f(x)\\) and \\(h(X)\\) is any function of \\(X\\), then: \\[E[h(X)] = \\int_{-\\infty}^{\\infty} h(x) \\cdot f(x) dx\\] Definition 4.4 The variance of a continuous random variable \\(X\\) with pdf \\(f(x)\\) is: \\[Var(X) = \\sigma^2 = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 \\cdot f(x) dx = E[(X-\\mu)^2] = E[X^2] -(E[X])^2\\] Definition 4.5 The standard deviation of a continuous random variable \\(X\\) is: \\[SD(X) = \\sigma = \\sqrt{Var(X)}\\] Theorem 4.4 If \\(X\\) is a continuous random variable and \\(Y=aX+b\\), then: \\[E[Y]=aE[X]+b \\text{ and } Var(Y)=a^2Var(X)\\] Example 4.5 Suppose \\(X\\) is a random variable with the following pdf: \\(f(x) = \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\frac{x}{2} &amp; 0 \\le x \\le 2 \\\\ 0 &amp; x &gt; 2\\\\ \\end{cases}\\) Determine \\(E[X]\\), \\(E[X^2]\\), \\(Var(X)\\), \\(SD(X)\\). 4.4 Normal Distribution Definition 4.6 A continuous random variable X is said to be Normal(\\(\\mu\\),\\(\\sigma^2\\)) or \\(\\mathcal{N}(\\mu,\\sigma^2)\\) random variable if the pdf of X is: \\[f(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(x-\\mu)^2/(2\\sigma^2)}, -\\infty &lt; x &lt; \\infty\\] If \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), then \\(E[X] = \\mu\\) and \\(Var(X) = \\sigma^2\\), that is, the expected value is \\(\\mu\\) and the variance is \\(\\sigma^2\\). Definition 4.7 The normal distribution with parameter values \\(\\mu=0\\) and \\(\\sigma=1\\) is called the standard normal distribution. Note: \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), F(x) is denoted \\(\\Phi(x)\\). There is no closed form solution for \\(\\Phi(x)\\) since\\ \\(\\Phi(x) = F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(x-\\mu)^2/(2\\sigma^2)} = (?).\\) Instead, we use a table called a standard normal table to assist in these calculations. Theorem 4.5 If \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), that is, X is a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\(Z = \\frac{X-\\mu}{\\sigma}\\) has a standard normal distribution. Theorem 4.6 If \\(Z \\sim \\mathcal{N}(0,1)\\), that is, X is a standard normal distribution, then \\(X = \\sigma \\cdot Z + \\mu\\) is a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Note: This normalization method allows us to calculate probability for any normal distribution using the standard normal table. (This will be covered in an upcoming example.) Theorem 4.7 The Empirical Rule states that if \\(X\\) is (approximately) normally distributed then: Approximately 68% of the values are within 1 SD of the mean. Approximately 95% of the values are within 2 SD of the mean. Approximately 99.7% of the values are within 3 SD of the mean. Example 4.6 Suppose the weights of house cats are approximately normally distributed with mean of 10 pounds and a standard deviation of 3 pounds. Between what two bounds do approximately 95% of house cat weights lie? Example 4.7 Suppose \\(Z \\sim \\mathcal{N}(0,1)\\), that is, \\(Z\\) is a standard normal distribution. Calculate the following and draw an accompanying picture: \\(P(Z &lt; 0.68)\\) \\(P(Z \\geq -1.21)\\) \\(P(0.43 \\le Z &lt; 0.68)\\) Suppose \\(P(Z &lt; c) = 0.89\\). Find \\(c\\). Example 4.8 Suppose the weights of house cats are approximately normally distributed with mean of 10 pounds and a standard deviation of 3 pounds. Calculate the following and draw an accompanying picture. \\(P(X&lt;15)\\) What is the probability that a randomly chosen house cat will weigh between 8 and 12 pounds? What is the \\(99^{th}\\) percentile of house cat weights? 4.5 The Exponential and Uniform Random Variables Definition 4.8 A continuous random variable X is said to be an Exponential(\\(\\lambda\\)) random variable or an exponential random variable with parameter \\(\\lambda &gt;0\\) if the pmf of X is: \\(f(x|\\lambda) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; x \\geq 0\\\\ 0 &amp; otherwise \\end{cases}\\) If \\(X \\sim exp(\\lambda)\\), then \\(E[X] = \\frac{1}{\\lambda}\\) and \\(Var(X) = \\frac{1}{\\lambda^2}\\). Example 4.9 Suppose \\(X \\sim exp(\\lambda)\\), find the CDF of X. Example 4.10 Show that if \\(X \\sim exp(\\lambda)\\), then \\(E[X] = \\frac{1}{\\lambda}\\). (Note: this requires integration by parts). 4.6 (Continuous) Uniform[a,b] Random Variable Definition 4.9 A continuous random variable X is said to be Uniform[\\(a\\),\\(b\\)] random variable if the pdf of \\(X\\) is: \\(f(x|a,b) = \\begin{cases} \\frac{1}{b-a} &amp; a \\le x \\le b \\\\ 0 &amp; otherwise \\\\ \\end{cases}\\) If \\(X \\sim \\text{Uniform(}a,b\\)) random variable, then \\(E[X] = \\frac{a+b}{2}\\) and \\(Var(X) = \\frac{(b-a)^2}{12}\\). Example 4.11 Show that if \\(X \\sim Uniform[a,b]\\), then \\(E[X] = \\frac{a+b}{2}\\). Example 4.12 Suppose your local bus shows up once every 60 minutes, but you don’t have a schedule, so you assume it’s equally likely to show up at any time in the next 60 minutes. Let \\(X\\) be the amount of time you have to wait for the bus. Sketch the pmf and calculate the probability that your bus shows up in the next 10 minutes. 4.7 R Companion for Chapter 4 Similar to last chapter, we can calculate probabilities for common continuous random variables in R and generate random data according to continuous distributions. R uses four prefixes to reference difference elements of a random variable. These are: p for “probability”, the cumulative distribution function (CDF) q for “quantile”, the inverse CDF d for “density”, the probability mass function (PMF) r for “random”, a random variable having the specified distribution Suffixes for continuous random variables include: norm (normal/Gaussian), exp (exponential), and t (t). Suppose \\(X \\sim \\mathcal{N}(\\mu=100,\\sigma=15)\\), the distribution often used to model IQ scores. Suppose we want to know the probability of a randomly chosen IQ score below 110. pnorm(q=110,mean=100,sd=15) ## [1] 0.7475075 In other words, about 75% of IQ scores are less than 110.\\ Let’s calculate the probability that a randomly chosen score is above 130. Note that \\(P(X&gt;130) = 1 - P(X \\leq 130)\\). 1-pnorm(q=130,mean=100,sd=15) ## [1] 0.02275013 In other words, about 2.3% of IQ scores are greater than 130.\\ We can also do inverse normal calculations using the prefix q which stands for quantile. Suppose we want to know the 90th percentile of IQ scores. We can calculate this using the qnorm function. qnorm(p=0.9,mean=100,sd=15) ## [1] 119.2233 This result shows that the 90th percentile of IQ scores corresponds to an approximate IQ of 119. In the second half of the course, we will be using the t-distribution frequently when dealing with statistical applications. Let’s generate 10,000 random values for a t-distribution with 20 degrees of freedom, i.e., \\(Z \\sim t(df=20)\\). set.seed(2020) rand.data = rt(n=10000,df=20) hist(rand.data) Based on the plot above, this distribution is approximately symmetric, bell-shaped, and unimodal.\\ Let’s calculate some summary statistics for this data. mean(rand.data) ## [1] -0.009077007 median(rand.data) ## [1] -0.01514699 The mean and median are both approximately equal to zero and since these values are nearly equal, we can confirm that the distribution is approximately symmetric. Finally, let’s calculate the probability in our simulated data so that \\(P(Z&gt;1)\\). num.events = sum(rand.data &gt; 1) prob = num.events / 10000; prob ## [1] 0.1623 Approximately 16% of our randomly generated t-distribution values are greater than 1. "],["multivariate-random-variables.html", "Chapter 5 Multivariate Random Variables 5.1 Joint Probability Distributions 5.2 Marginal Probability Functions 5.3 Independent Random Variables 5.4 Conditional Distributions 5.5 Expected Values, Covariance, and Correlation 5.6 Bivariate Normal Distribution 5.7 R Companion for Chapter 5", " Chapter 5 Multivariate Random Variables These notes will explore pairs of random variables that may be related. To understand the relationship between these variables, we will start by defining joint probability distributions. Following that, we will examine the concepts of covariance and correlation to quantify and describe the nature of the relationship between the two variables. 5.1 Joint Probability Distributions Definition 5.1 Let \\(X\\) and \\(Y\\) be two discrete random variables defined on the sample space \\(\\Omega\\) of an experiment. The joint probability mass function, \\(p(x,y)\\), is defined for each pair of numbers (x,y) by: \\[p(x,y) = P(X=x \\text{ and } Y=y)\\] Note: \\(p(x,y) \\geq 0\\) and \\(\\sum_x \\sum_y p(x,y) = 1\\) Definition 5.2 Let \\(X\\) and \\(Y\\) be two continuous random variables defined on the sample space \\(\\Omega\\) of an experiment. The joint probability density function \\(f(x,y)\\) for variables \\(X\\) and \\(Y\\) is a function satisfying: \\(f(x,y) \\geq 0\\) \\(\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) dy dx = 1\\) Note: For any two-dimensional set A, \\(P((X,Y) \\in A) = {\\int \\int}_A f(x,y) dy dx\\). Note: If A is a two-dimensional rectangle \\(\\{(x,y): a \\le x \\le b, c \\le y \\le d\\}\\), then: \\(P((X,Y) \\in A) = \\int_a^b \\int_c^d f(x,y) dy dx\\) Example 5.1 Let \\(f(x,y) = c xy, 0 \\leq x,y \\leq 1\\). Find \\(c\\) so that \\(f(x,y)\\) is a valid joint pdf. 5.2 Marginal Probability Functions Definition 5.3 The marginal probability mass function of a discrete random variable \\(X\\) with joint pmf \\(p(x,y)\\) is denoted by \\(p_X(x)\\) and is given by: \\(p_X(x) = \\sum_y p(x,y)\\) Definition 5.4 The marginal probability density function of a continuous random variable X with joint pdf \\(f(x,y)\\) is denoted by \\(f_X(x)\\) and is given by: \\(f_X(x) = \\int_y f(x,y) dy\\) Example 5.2 An apartment complex has the following joint distribution of \\(X\\) bedrooms and \\(Y\\) bathrooms. Determine the marginal pmfs and calculate the probability a randomly selected apartment has more bedrooms than bathrooms (i.e., \\(P(X&gt;Y)\\)). Y &amp;#124; X 1 2 3 \\(\\Sigma\\) 1 0.3 0.3 0 2 0 0.3 0.1 \\(\\Sigma\\) Example 5.3 Let \\(X\\) and \\(Y\\) have the joint pdf, \\(f(x,y)\\), as follows. \\(f(x,y) = \\begin{cases} \\frac{4}{3}(xy + x) &amp; 0 \\leq x \\leq 1, 0 \\leq y \\leq 1\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\) ::: Confirm \\(f(x,y)\\) is a valid pdf Determine the marginal pdfs Calculate \\(P(X&lt;1/2,Y&lt;1/2)\\) 5.3 Independent Random Variables Definition 5.5 Two discrete random variables are independent if \\(p(x,y) = p_X(x) \\cdot p_Y(y)\\). Definition 5.6 Two continuous random variables are independent if \\(f(x,y) = f_X(x) \\cdot f_Y(y)\\). Definition 5.7 If two random variables are not independent, they are dependent. Note: You can also check independence by seeing if the conditional distribution equals the marginal distribution, i.e., \\(p_{Y|X}(y|x) = p_{Y}(y)\\) OR \\(p_{X|Y}(x|y) = p_X(x)\\). Example 5.4 Let \\(X\\) and \\(Y\\) have the joint pdf, \\(f(x)\\), as follows. Are \\(X\\) and \\(Y\\) independent? \\(f(x,y) = \\begin{cases} \\frac{4}{3}(xy + x) &amp; 0 \\leq x \\leq 1, 0 \\leq y \\leq 1\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\) Example 5.5 An apartment complex has the following joint distribution of \\(X\\) bedrooms and \\(Y\\) bathrooms. Are \\(X\\) and \\(Y\\) independent? Y &amp;#124; X 1 2 3 \\(\\Sigma\\) 1 0.3 0.3 0 2 0 0.3 0.1 \\(\\Sigma\\) 5.4 Conditional Distributions Definition 5.8 Let \\(X\\) and \\(Y\\) be two discrete distributions with joint pmf \\(p(x,y)\\) and marginal pmf of \\(X\\) \\(p_X(x)\\). Then for any \\(X\\) value of \\(x\\) for which \\(p_X(x) &gt; 0\\), the ***conditional probability mass function} of \\(Y\\) given that \\(X=x\\) is: \\[p_{Y|X}(y|x) = \\frac{p(x,y)}{p_X(x)} \\] Definition 5.9 Let \\(X\\) and \\(Y\\) be two continuous distributions with joint pdf \\(f(x,y)\\) and marginal pdf of \\(X\\) \\(f_X(x)\\). Then for any \\(X\\) value of \\(x\\) for which \\(f_X(x) &gt; 0\\), the ***conditional probability density function} of \\(Y\\) given that \\(X=x\\) is: \\[f_{Y|X}(y|x) = \\frac{f(x,y)}{f_X(x)} \\] Example 5.6 For the apartment example, determine the conditional pmf \\(p_{Y|X}(y|x=2)\\). The joint pmf, \\(p_{X,Y}(x,y)\\), is provided below. Y &amp;#124; X 1 2 3 \\(\\Sigma\\) 1 0.3 0.3 0 2 0 0.3 0.1 \\(\\Sigma\\) Example 5.7 Let \\(X\\) and \\(Y\\) have the joint pdf, \\(f(x,y)\\), as follows. Determine the conditional pdf, \\(f_{Y|X}(y|x)\\). \\(f(x,y) = \\begin{cases} \\frac{4}{3}(xy + x) &amp; 0 \\leq x \\leq 1, 0 \\leq y \\leq 1\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\) 5.5 Expected Values, Covariance, and Correlation Theorem 5.1 Let X and Y be jointly distributed pmf \\(p(x,y)\\) (if X and Y are discrete) or pdf \\(f(x,y)\\) (if X and Y are continuous). The expected value of the function \\(h(X,Y)\\), \\(E[h(X,Y)]\\), is given by: \\[E[h(X,Y)] = \\sum_x \\sum_y h(x,y) \\cdot p(x,y) \\text{ (if X and Y are discrete)} \\] \\[E[h(X,Y)] = \\int_x \\int_y h(x,y) \\cdot f(x,y) dy dx \\text{ (if X and Y are continuous)} \\] Definition 5.10 The covariance between two random variables is given by: \\[ Cov(X,Y) = E[XY] - E[X]E[Y]\\] Definition 5.11 The correlation coefficient between two random variables is given by: \\[Corr(X,Y) = \\rho_{XY} = \\rho = \\frac{Cov(X,Y)}{\\sigma_X \\cdot \\sigma_Y}\\] Theorem 5.2 For any two random variables \\(X\\) and \\(Y\\), \\(-1 \\le \\rho_{XY} \\le 1\\). Theorem 5.3 If \\(X\\) and \\(Y\\) are independent then \\(\\rho_{XY} = 0\\) Note: \\(\\rho_{XY} = 0\\) does not imply that two random variables are independent! Example 5.8 For the apartment example, determine \\(E[X]\\), \\(E[Y]\\), \\(E[XY]\\), and \\(Cov(X,Y)\\). The joint pmf, \\(p_{X,Y}(x,y)\\), is provided below. Y &amp;#124; X 1 2 3 \\(\\Sigma\\) 1 0.3 0.3 0 2 0 0.3 0.1 \\(\\Sigma\\) Theorem 5.4 Let X and Y be random variables and a and b be constants. \\(E[aX+bY] = aE[X] + bE[Y]\\) \\(Var(aX+bY) = a^2 Var(X) + b^2 Var(Y) + 2ab \\cdot Cov(X,Y)\\) \\(Var(aX+bY) = a^2 Var(X) + b^2 Var(Y)\\), if X and Y are independent 5.6 Bivariate Normal Distribution The bivariate normal distribution of (\\(X\\),\\(Y\\)) has the following joint probability distribution function: \\[f(x,y) = \\frac{1}{2 \\pi \\sigma_1 \\sigma_2 \\sqrt{1-\\rho^2}} \\cdot exp\\Bigg[-\\frac{1}{2(1-\\rho^2)}\\bigg(\\Big(\\frac{x-\\mu_X}{\\sigma_X}\\bigg)^2 + 2 \\rho \\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_1 \\sigma_2} + \\Big(\\frac{y-\\mu_Y}{\\sigma_Y}\\Big)^2 \\bigg) \\Bigg] \\] \\[-\\infty &lt; x &lt; \\infty , -\\infty &lt; y &lt; \\infty\\] where \\(E[X] = \\mu_X\\), \\(Var(X) = \\sigma_X^2\\), \\(E[Y] = \\mu_Y\\), \\(Var(Y) = \\sigma_Y^2\\), \\(\\rho = Corr(X,Y)\\) Bivariate Normal Distribution We won’t work directly with the multivariate normal distribution, but there are some useful properties of normal random variables which we will use. Theorem 5.5 If \\(X \\sim \\mathcal{N}(\\mu_X,\\sigma^2_X)\\) and \\(Y \\sim \\mathcal{N}(\\mu_Y,\\sigma^2_Y)\\), then: \\(X+Y \\sim \\mathcal{N}(\\mu_X+\\mu_Y,\\sigma^2_X+\\sigma^2_Y+2Cov(X,Y))\\), that is, the sum of normal random variables is a normal random variable. Theorem 5.6 If \\(X \\sim \\mathcal{N}(\\mu_X,\\sigma^2_X)\\) and \\(Y \\sim \\mathcal{N}(\\mu_Y,\\sigma^2_Y)\\) and X and Y are independent, then: \\(X+Y \\sim \\mathcal{N}(\\mu_X+\\mu_Y,\\sigma^2_X+\\sigma^2_Y)\\). Example 5.9 Suppose \\(X\\) and \\(Y\\) are independent normal random variables with means \\(\\mu_X=2\\) and \\(\\mu_Y=3\\) and variances \\(\\sigma_X^2=1\\) and \\(\\sigma_Y^2=4\\). Determine the distribution of \\(Z = 4X-Y\\). 5.7 R Companion for Chapter 5 Example 5.10 Let’s return to the Hubble dataset. Let’s load the dataset into R and then plot the data. Note the plot parameter pch=16 fills in the data points for a nicer looking plot. hubble = read_csv(file = &quot;hubble.csv&quot;) plot(Velocity~Distance,data=hubble, pch=16) There appears to be a positive relationship between Distance and Velocity. We can calculate the correlation and covariance between the two variables as well. cor(hubble$Distance,hubble$Velocity) ## [1] 0.789032 cov(hubble$Distance,hubble$Velocity) ## [1] 189.159 The correlation is +0.789 indicating a positive, moderately strong linear relationship between Distance and Velocity. The covariance is 189.159 which is less interpretable since covariance is dependent on the units of measurement. For this reason, we typically prefer to use correlation rather than covariance. Example 5.11 The mtcars dataset is provided in the base version of R. To find out more info about this dataset, you can get details by calling ?mtcars. # get info on the mtcars dataset ?mtcars In the side window of R Studio, it should display the following description after running ?mtcars. The data were extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). Let’s load the dataset and take a look at the first few values. data(mtcars) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 In the code below, we extract just these two variables from mtcars and create a new variable m. From this new variable m, we can construct a two-way table of counts for the combinations of cyl and gear. # construct a two-way table of counts for cyl and gear m.table = table(mtcars$cyl, mtcars$gear) m.table ## ## 3 4 5 ## 4 1 8 2 ## 6 2 4 1 ## 8 12 0 2 We can also construct the joint probability mass function by dividing our table by the total number of observations. # calculate the number of total observations n = sum(m.table); n ## [1] 32 # construct the joint pmf m.pmf = m.table/n; m.pmf ## ## 3 4 5 ## 4 0.03125 0.25000 0.06250 ## 6 0.06250 0.12500 0.03125 ## 8 0.37500 0.00000 0.06250 Using the table, we can see that P(gear=3 and cyl=8) = 0.375 for example. What is P(gear=4 and cylinder = 4)? The middle entry of the table tells us this value is 0.125. In other words, 12.5% of the cars in this dataset have 4 gears and 4 cylinders. Next, we are going to look at the joint probability mass function (PMF) of the variables cyl} and gear}. We can get the marginal PMFs by summing across rows and columns of the joint pmf. # marginal pmf of gear colSums(m.pmf) ## 3 4 5 ## 0.46875 0.37500 0.15625 # marginal pmf of cyl rowSums(m.pmf) ## 4 6 8 ## 0.34375 0.21875 0.43750 From the tables above, you should see that P(gear=3)=0.46875 and P(cyl=4)=0.34375. Finally, we can get the correlation as follows. Note that gear and cyl have a negative, moderate linear relationship. cor(mtcars$cyl,mtcars$gear) ## [1] -0.4926866 "],["sampling-distributions-central-limit-theorem-and-estimation.html", "Chapter 6 Sampling Distributions, Central Limit Theorem, and Estimation 6.1 Statistics and Their Distributions 6.2 The Distribution of the Sample Mean 6.3 Several General Concepts of Point Estimation 6.4 Methods of Point Estimation 6.5 R Companion for Chapter 6", " Chapter 6 Sampling Distributions, Central Limit Theorem, and Estimation In this chapter, we will explore sampling distributions, focusing on the variability of sample statistics, which is crucial for inference. We will then dive into the Central Limit Theorem (CLT), which demonstrates how the distribution of sample means approaches normality as the sample size grows, forming the backbone of many statistical methods. The Law of Large Numbers (LLN) will also be discussed, showing how larger samples tend to provide more accurate estimates of population parameters. Finally, we will introduce point estimation, highlighting how to derive reliable population parameter estimates through sample data and discussing properties such as bias and standard error. 6.1 Statistics and Their Distributions Definition 6.1 A statistic is any quantity whose value can be calculated from sample data. Definition 6.2 The random variables \\(X_1, X_2, \\ldots, X_n\\) are said to form a random sample of size \\(n\\) if \\(X_i\\) are independent and identically distributed (IID). In chapter 1, we came across a few statistics. We can measure the center of a sample with the sample mean, \\(\\bar{x}\\) and the sample median, \\(\\tilde{x}\\). If we want to describe the spread of a sample, there are the sample variance (\\(s^2\\)) and sample standard deviation (\\(s\\)). In fact, since each random sample could potentially be different, sample statistics have a probability distribution associated with them. Example 6.1 Radon-222 has a half-life (median decay time) of 3.8 days and the the time between particles decaying can be modeled as an exponential(\\(\\lambda = 0.182\\)) random variable. This random variable has a mean time of 5.5 days between decays. Simulate 10,000 Radon-222 decay times and plot the data as a histogram. set.seed(2020) sims = rexp(n=10000,rate=0.182) hist(sims,main=&quot;10,000 simulations of Radon-222 decay times&quot;) mean(sims) ## [1] 5.574666 var(sims) ## [1] 30.55352 Simulate four exponential(\\(\\lambda=0.182\\)) random variables five times and calculate the sample mean of each sample. set.seed(1) sample1 = rexp(n=4,rate=0.182); mean1 = mean(sample1) sample2 = rexp(n=4,rate=0.182); mean2 = mean(sample2) sample3 = rexp(n=4,rate=0.182); mean3 = mean(sample3) sample4 = rexp(n=4,rate=0.182); mean4 = mean(sample4) sample5 = rexp(n=4,rate=0.182); mean5 = mean(sample5) \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(\\bar{x}\\) 4.1493507 6.4925427 0.8005864 0.7681058 3.0526464 2.395981 15.906421 6.755835 2.965290 7.005882 5.255865 0.807945 7.641402 4.186977 4.473047 6.800020 24.307331 5.794193 5.688154 10.647424 10.307886 3.597509 1.851283 3.233405 4.747521 Compute the mean and variance of the five sample means. means = c(mean1,mean2,mean3,mean4,mean5) mean(means) ## [1] 5.985304 var(means) ## [1] 8.799113 Repeat the experiment 10,000 times, that is, simulate 10,000 samples of 4 exponential(\\(\\lambda=0.182\\)) random variables. Create a histogram of the sample means and calculate the mean and variance of the sample means. set.seed(2020) nsims = 10000 means = rep(0,nsims) simdata = rep(0,4) for(i in 1:nsims){ simdata = rexp(n=4,rate=0.182) means[i] = mean(simdata) } hist(means,main=&quot;10,000 simulations of sample means of 4 Radon-222 decay times&quot;) mean(means) ## [1] 5.531811 var(means) ## [1] 7.719361 Simulate 10,000 exponential(\\(\\lambda=0.182\\)) random variables and plot the sample mean as a function of sample size. set.seed(1) data = rexp(n=10000,rate=0.182) means = cumsum(data) / seq_along(data) plot(x=1:10000,y=means,type=&quot;l&quot;,xlab=&quot;n&quot;,ylab=&quot;Sample Mean&quot;, main=&quot;Sample mean as a function of sample size&quot;) abline(h = 1/.182,lty=2) Theorem 6.1 The Law of Large Numbers states that there is a tendency for the sample mean, \\(\\bar{x}\\), to approach the population mean, \\(\\mu\\), as \\(n \\to \\infty\\). Specifically, if \\(|E[X]| = \\mu &lt; \\infty\\), then \\(\\lim_{n\\to\\infty} 1/n \\sum_{n=1}^{\\infty} x_i = \\lim_{n\\to\\infty} \\bar{x} = \\mu\\). 6.2 The Distribution of the Sample Mean Example 6.2 Suppose a historical estimate of the half-life of Radon-222 was 6 days (with corresponding rate \\(\\lambda = 0.1155\\)) which in turn has an expected value of 8.66 days. To test the accuracy of this claim, we measured the decay times for 50 Radon-222 particles and found a sample mean of \\(\\bar{x} = 5.67\\) and sample variance of \\(s^2 = 28.94\\). Do we have enough information to reject the claim? To answer this question, we need to know the distribution of \\(\\bar{X}\\) and specifically, we would like to calculate the probability of observing such a sample mean assuming the historical estimate is accurate. Theorem 6.2 Let \\(X_1, X_2, \\ldots, X_n\\) be a random sample from a distribution with mean value \\(\\mu\\) and standard deviation \\(\\sigma\\) and let \\(\\bar{X} = \\frac{1}{n} \\sum X_i\\). Then: \\(E[\\bar{X}] = \\mu\\) \\(Var(\\bar{X}) = \\sigma_{\\bar{X}}^2 = \\frac{\\sigma^2}{n}\\) \\(\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) Example 6.3 Show that \\(Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\). Definition 6.3 If \\(\\hat{\\theta}\\) is an estimator for \\(\\sigma\\), then the standard error of \\(\\hat{\\theta}\\) is: \\(SE(\\hat{\\theta}) = \\sqrt{Var(\\hat{\\theta})}\\). It is the magnitude of a typical or representative deviation between an estimate and the value of \\(\\theta\\). If the standard error is a function of unknown parameters, then we can estimate it with the estimated standard error which is denoted \\(s_{\\hat{\\theta}}\\) Example 6.4 If \\(X_1, X_2, \\ldots, X_{50}\\) are iid Uniform[10,20] random variables, then determine \\(E[\\bar{X}]\\) and \\(SE(\\bar{X})\\). Note: The symbol, \\(\\stackrel{\\cdot}{\\sim}\\), stands for “approximately distributed as”. Theorem 6.3 Central Limit Theorem: Let \\(X_1, X_2, \\ldots, X_n\\) be a random sample from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). If n is sufficiently large, \\(\\bar{X}\\) has approximately a normal distribution with mean \\(\\mu_{\\bar{X}} = \\mu\\) and variance \\(\\sigma_{\\bar{X}}^2 = \\sigma^2/n\\). In other words, \\(\\bar{X} \\stackrel{\\cdot}{\\sim} \\mathcal{N}(\\mu,\\sigma^2)\\). This result is nontrivial and extremely useful! It allows us to determine how likely or unlikely a sample mean is if we are given a hypothesized population mean. Illustration: Example 6.5 Suppose that \\(X_1, X_2, \\ldots, X_{50}\\) is a random sample from an exponential(\\(\\lambda = 0.1155\\)) random variable (the historical claim). Determine the approximate sampling distribution of \\(\\bar{X}\\). Example 6.6 Class exercise (if time allows): We will collect data based on the following experiments. Plot a histogram for each instance. Roll your die once and record your number: \\(\\rule{1.5cm}{0.15mm}\\) Roll your die twice and record the average of the two die rolls: \\(\\rule{1.5cm}{0.15mm}\\) Roll your die ten times and record the average of the ten die rolls: \\(\\rule{1.5cm}{0.15mm}\\) Example 6.7 Simulate N fair die rolls 10,000 times. Here we will do the simulation for N=5, N=10, and N=50. set.seed(2020) xbar1 = rep(0,10000) # initialize this vector to zeros xbar2 = rep(0,10000) # initialize this vector to zeros xbar3 = rep(0,10000) # initialize this vector to zeros # 10,000 simulated die rolls for N=5,10,50 for( i in 1:10000 ){ x1 = sample(1:6,5,replace=T) xbar1[i] = mean(x1) x2 = sample(1:6,10,replace=T) xbar2[i] = mean(x2) x3 = sample(1:6,50,replace=T) xbar3[i] = mean(x3) } par(mfrow=c(1,3)) hist(xbar1,main=&quot;N=5&quot;,xlim=c(1,6),prob=TRUE) lines(density(xbar1),lwd=2,lty=2) hist(xbar2,main=&quot;N=10&quot;,xlim=c(1,6),prob=TRUE) lines(density(xbar2),lwd=2,lty=2) hist(xbar3,main=&quot;N=50&quot;,xlim=c(1,6),prob=TRUE) lines(density(xbar3),lwd=2,lty=2) Example 6.8 Suppose the decay time of Radon-222 follows an exponential distribution with \\(\\lambda = 0.1155\\) decays/day (the historical claim). What is the probability that a random sample of 50 Radon particles results in a sample mean of less than 5.67 days/decay? Do you believe the historical claim? Example 6.9 Suppose \\(X_1, X_2, \\ldots, X_{50}\\) are iid Uniform[10,20] random variables. Determine the sampling distribution of \\(\\bar{X}\\) and calculate \\(P(\\bar{X}&lt;14)\\). 6.3 Several General Concepts of Point Estimation Definition 6.4 A point estimate of a parameter \\(\\theta\\) is a single number that can be regarded as a sensible value for \\(\\theta\\) and is often denoted with a hat (i.e., \\(\\hat{\\theta}\\)). A point estimate is obtained by selecting a suitable statistic and computing its value from the given sample data. The selected statistic, \\(\\hat{\\theta}\\), is called the point estimator of \\(\\theta\\). Example 6.10 \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) \\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i -\\bar{X})^2\\) Definition 6.5 The bias of a point estimator \\(\\hat{\\theta}\\) for a parameter \\(\\theta\\) is defined as \\(Bias[\\hat{\\theta}] = E[\\hat{\\theta}] - \\theta\\). Definition 6.6 A statistic \\(\\hat{\\theta}\\) is called an unbiased estimator of the parameter \\(\\theta\\) if \\(E[\\hat{\\theta}] = \\theta\\). Example 6.11 \\(\\bar{X}\\) is an unbiased estimator for \\(\\mu\\). Example 6.12 Which of the following two estimators for \\(\\sigma^2\\) is unbiased for \\(\\sigma^2\\)? \\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i -\\bar{X})^2\\) \\(\\gamma = \\frac{1}{n} \\sum_{i=1}^n (X_i -\\bar{X})^2\\) Definition 6.7 The mean squared error (MSE) of an estimator \\(\\hat{\\theta}\\) of a parameter \\(\\theta\\) is defined by \\(E[(\\hat{\\theta} - \\theta)^2] = Var + Bias^2\\). Typically, unbiased estimators are preferred though some advanced methods consider biased estimators that minimize MSE. Example 6.13 Suppose \\(X_1, X_2, \\ldots, X_n\\) are exponential(\\(\\lambda\\)) random variables. Is \\(\\bar{X}\\) an unbiased estimator for \\(\\lambda\\)? Example 6.14 Suppose \\(X_1, X_2, \\ldots, X_n\\) are Poisson(\\(\\lambda\\)) random variables. Is \\(\\bar{X}\\) an unbiased estimator for \\(\\lambda\\)? 6.4 Methods of Point Estimation There are a variety of ways to obtain a point estimate. These methods include: Method of Moments Maximum Likelihood Estimation Bayesian Estimation While we won’t cover how these estimation methods are completed, we will compare different estimators. Each of these methods could produce a different point estimate. Example 6.15 Suppose \\(X_1, X_2, X_3\\) are a random sample of \\(\\mathcal{N}(\\mu=10,\\sigma^2=4)\\). Four estimators are given below. Determine if each estimator is unbiased and calculate its standard error. Which of the four estimators is preferred for estimating \\(\\mu\\)? \\(\\hat{Y_1} = \\frac{X_1 + X_2 + X_3}{3}\\) \\(\\hat{Y_2} = \\frac{X_1}{2}+\\frac{X_2}{4}+\\frac{X_3}{4}\\) \\(\\hat{Y_3} = X_1\\) \\(\\hat{Y_4} = \\frac{X_1+X_2+X_3}{2}\\) 6.5 R Companion for Chapter 6 Example 6.16 What does the distribution of the sample means of independent (continuous) uniform random variables look like? The Central Limit Theorem tells us that for a large sample size, the distribution of such means will be approximately normally distributed. Let’s see this in action. Suppose we know a bus arrives once per hour at a designated stop but we don’t know when the next bus is and we don’t know when the last bus came. We will model this as a continuous uniform[0,60] random variable. In other words, the next bus could come in anywhere from 0 to 60 minutes with all possibilities being equally likely. In the code below, we simulate 10,000 uniform[0,60] random variables and look at the histogram of results. # Generate 10,000 uniform[0,60] random varibles set.seed(2020) bus = runif(10000,0,60) hist(bus) The mean and variance of this sampling distribution is calculated below. mean(bus) ## [1] 29.79179 var(bus) ## [1] 298.1569 From chapter 4, we know that if \\(X \\sim Uniform[a,b]\\), then \\(E[X] = \\frac{b+a}{2}\\) and \\(Var(X) = \\frac{(b-a)^2}{12}\\). For a uniform[0,60] random variable, we get the theoretical values of \\(E[X] = \\frac{60+0}{2}=30\\) and \\(Var(X)=\\frac{(60-0)^2}{12}=300\\). Our simulated values are very close to these theoretical values. Now, let’s suppose we have to wait for two buses, each being modeled as an independent uniform[0,60]. Let’s look at the sampling distribution for the mean of the two bus wait times. set.seed(2020) bus2 = rep(0, 10000) # initialize this vector to zeros # 10,000 simulations of 2 bus waits for (i in 1:10000){ x1 = runif(2,0,60) # n=2 bus2[i] = mean(x1) } mean(bus2) ## [1] 29.90972 var(bus2) ## [1] 149.6154 hist(bus2) Notice that the average of two bus waits is no longer uniform and instead has more of a triangular shape. Our simulations of two bus waits had a mean of 30.05955 and variance of 151.2226. From chapter 6, we know that \\(E[\\bar{X}]=\\mu\\) and \\(Var(\\bar{X})=\\frac{\\sigma^2}{n}\\). For this simulation, the theoretical values for these quantities are \\(E[\\bar{X}]=\\mu=30\\) and \\(Var(\\bar{X})=\\frac{\\sigma^2}{n}=\\frac{300}{2}=150\\). Again, our simulations give values very close to the theoretical values. Finally, let’s suppose that we repeat the experiment for 30 days straight and look at the distribution of sample means of 30 days. We will simulate these 30 day periods 10,000 times. set.seed(2020) bus30 = rep(0, 10000) # initialize this vector to zeros # 10,000 simulations of 2 bus waits for (i in 1:10000){ x2 = runif(30,0,60) # n=30 bus30[i] = mean(x2) } mean(bus30) ## [1] 29.97879 var(bus30) ## [1] 10.19619 hist(bus30) Our simulations of 30 bus waits had a mean of 30.00598 and variance of 10.08975. The theoretical values for this simulation are \\(E[\\bar{X}]=\\mu = 30\\) and \\(Var(\\bar{X})=\\frac{\\sigma^2}{n} = \\frac{300}{30}=10\\). Notice that the distribution of sample means is narrower for n=30 than n=2 and the variance is lower when a larger sample size is used. Also note that the distribution is somewhat bell-shaped. In the coming chapters on Inferential Statistics, we will use the fact that the Central Limit Theorem states that the sampling distribution of sample means is approximately normally distributed. "],["confidence-intervals-for-one-population-parameter.html", "Chapter 7 Confidence Intervals for One Population Parameter 7.1 Basic Concepts of Confidence Intervals 7.2 Large-Sample Confidence Intervals for a Population Mean and Proportion 7.3 Intervals Based on a Normal Population Distribution", " Chapter 7 Confidence Intervals for One Population Parameter In this chapter, we will explore how to construct confidence intervals for one-parameter population values, such as the population mean or proportion. In Chapter 6, we discussed how to derive a point estimate for a population parameter. Since each sample may produce a different point estimate, it’s essential to account for the inherent randomness in this process. Confidence intervals provide a way to quantify the uncertainty associated with these estimates by creating a range of plausible values for the parameter, rather than relying on a single estimate. This approach gives us a more reliable method to assess the precision of our estimates. 7.1 Basic Concepts of Confidence Intervals In Chapter 6, we examined how to find a point estimate for an unknown population parameter. When reporting point estimates, it is important to note the amount of uncertainty associated with the point estimate. This is especially key when we want to do Statistical Inference. We will examine two methods of Statistical Inference: Confidence Intervals Hypothesis Tests Definition 7.1 A confidence interval is a range of values, derived from sample data, that reflects the uncertainty associated with an estimate of an unknown population parameter. It provides an interval within which the parameter is likely to fall, with a specified level of confidence (e.g., 95%). The confidence level indicates the probability that the interval will contain the true parameter in repeated sampling. There are many flavors of confidence intervals: Some are one-sided while others are two-sided Some assume population variance is known while others do not Some assume a higher confidence level while others assume a lower confidence level General Form of a Confidence Interval (Two-Sided) Definition 7.2 \\(\\alpha\\) is the probability that a confidence interval from repeated sampling does not contain the true population parameter. It represents the likelihood of the confidence interval failing to capture the parameter in repeated samples, and is equal to \\(1 - \\text{confidence level}\\). Illustration Example 7.1 Suppose we collect a random sample and use it to create a confidence interval. Further, suppose that we do this process 100 times and create 100 confidence intervals. Deriving a Confidence Interval Example 7.2 Suppose we want to create a two-sided 95% confidence interval for \\(\\mu\\). Writing and Interpreting a Confidence Interval Here’s how to write confidence intervals: Here’s how to interpret confidence intervals: 7.2 Large-Sample Confidence Intervals for a Population Mean and Proportion The Central Limit Theorem states that \\(Z = (\\bar{X}-\\mu)/(\\sigma/\\sqrt{n})\\) has an approximately standard normal distribution. We can use this result to produce a confidence interval since \\(P(-z_{\\alpha/2} &lt; \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} &lt; z_{\\alpha/2} ) \\approx 1 - \\alpha\\). Theorem 7.1 If our random sample comes from a normal distribution and \\(\\sigma\\) is known, then a \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\mu\\) is given by: \\({\\bar{x} \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}}\\) If \\(n\\) is large, then we don’t require normality. We will come to this case in the next section. This type of confidence interval can also be called a large-sample confidence interval. Example 7.3 A random sample of 16 CPU chips has a total of 342 defects. Suppose we know the population standard deviation is 4.52 and also that the number of defects/chip is approximately normally distributed. Calculate a point estimate for \\(\\mu\\), the true mean number of defects/chip. Calculate a 95% CI for \\(\\mu\\). Example 7.4 Suppose that GPAs are approximately normally distributed with population variance \\(\\sigma^2=1.2\\). If we take a random sample of 12 students and find a sample mean of 2.87. Construct a 90% confidence interval for \\(\\mu\\). 7.3 Intervals Based on a Normal Population Distribution In this section, we will assume that both \\(\\mu\\) and \\(\\sigma\\) are unknown. Recall that the Central Limit Theorem stated that \\(Z = (\\bar{X}-\\mu)/(\\sigma/\\sqrt{n})\\) has an approximately standard normal distribution. If \\(\\sigma\\) is unknown, then \\(t = (\\bar{X}-\\mu)/(s/\\sqrt{n})\\) does not have a normal distribution. Definition 7.3 The t-distribution is a probability distribution used in statistics that is similar to the normal distribution but has heavier tails. It is typically used when estimating population parameters when the sample size is small and/or the population standard deviation is unknown. The t-distribution becomes closer to the normal distribution as the sample size increases. \\(t_{k}\\) denotes a t-distribution with \\(k\\) degrees of freedom. Properties of the t-distribution Each \\(t_{k}\\) curve is bell-shaped and centered at zero. Each \\(t_{k}\\) curve has more spread than the standard normal curve. As \\(k\\) increases, the spread of the corresponding \\(t_{k}\\) curve decreases. As \\(k \\to \\infty\\), the sequence of \\(t_{k}\\) curves approaches the standard normal curve. Example 7.5 Find \\(t^*\\) such that the area under the curve to the right of \\(t^*\\) is 0.025 for \\(df=10\\). Example 7.6 Find \\(t^*\\) such that the area under the curve to the left of \\(t^*\\) is 0.05 for \\(df=14\\). Example 7.7 Find the two values, \\(t_1\\) and \\(t_2\\), such that the area under the curve is the middle 95% for \\(df=24\\). Example 7.8 Suppose \\(t^*=2.1\\) and \\(df=9\\). Calculate \\(P(t(9) \\geq t^*)\\). Theorem 7.2 If \\(\\bar{X}\\) is the mean of a random sample of size n from an (approximately) normal distribution with mean \\(\\mu\\) and sample standard devitation \\(s\\), then \\(t = (\\bar{X}-\\mu)/(s/\\sqrt{n})\\) follows a t-distribution with (n-1) degrees of freedom. Theorem 7.3 Let \\(\\bar{x}\\) and s be the sample mean and sample standard deviation from a random sample from a (approximately) normal population with mean \\(\\mu\\). Then a \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\mu\\) is given by: \\({\\bar{x} \\pm t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\\text{, df=n-1}}\\) Example 7.9 Suppose a random sample of 30 Hummers have a mean of 9.8 miles/gallon (MPG) and a sample standard deviation of 2.2. Compute a 95% confidence interval for \\(\\mu\\). Example 7.10 Suppose a random sample of 17 Priuses have a mean of 52.3 MPG and variance of 14.9. Construct a 99% confidence interval for \\(\\mu\\). "],["hypothesis-tests-for-one-population-parameter.html", "Chapter 8 Hypothesis Tests for One Population Parameter 8.1 Basic Concepts of Hypothesis Testing 8.2 Tests About a Population Mean, \\(\\mu\\) 8.3 Tests Concerning a Population Proportion, \\(p\\) 8.4 R Companion for Chapter 8", " Chapter 8 Hypothesis Tests for One Population Parameter In the previous chapter, we explored Statistical Inference through the application of confidence intervals, which allowed us to estimate population parameters within a range of plausible values. In this chapter, we will turn our attention to hypothesis testing, another fundamental approach in Statistical Inference. Hypothesis testing provides a structured framework to evaluate specific claims or assumptions about a population parameter by analyzing sample data. Through this method, we can determine whether the evidence supports or refutes a given hypothesis, helping us make informed decisions in the presence of uncertainty. 8.1 Basic Concepts of Hypothesis Testing Example 8.1 Imagine a company claims that its employees spend, on average, 5 hours per week using social media during work hours. The management is concerned that this may be affecting productivity and wants to verify if the average time is indeed 5 hours. To test this claim, a random sample of employees is surveyed, and the average time spent on social media is calculated. By formulating and testing a hypothesis, the company can assess whether the claim holds up under scrutiny or if the evidence suggests a different average usage. One way to test this claim would be to collect a sample and see if 5 is contained in the confidence interval for \\(\\mu\\). Another approach to testing this claim is to again collect a sample and see how consistent that sample is with initial hypothesis of \\(\\mu=5\\). This is called hypothesis testing. Goal of Hypothesis Testing: Test competing claims about a population parameter. Definition 8.1 The null hypothesis, denoted \\(H_0\\), represents the initial claim or assumption that is presumed to be true in the absence of evidence to the contrary. Definition 8.2 The alternative hypothesis, denoted \\(H_a\\), is the statement that contradicts \\(H_0\\), often described as the opposite of the null hypothesis, and represents what we are testing for in the analysis. The null hypothesis will be rejected in favor of the alternative hypothesis if the sample evidence suggests that \\(H_0\\) is false. The two possible conclusions of a hypothesis test are Reject \\(H_0\\) or Fail to Reject \\(H_0\\). Formulating Hypotheses \\(H_0\\) contains \\(=\\), \\(\\leq\\), \\(\\geq\\). \\(H_a\\) contains \\(\\neq\\), \\(&gt;\\), \\(&lt;\\) Usually, the claim we are attempting to show is more plausible is \\(H_a\\) Example 8.2 Jorge claims that homework for this class takes, on average, six hours per week. Aaron doesn’t think the average is six hours, so he collects a random sample of eight students and records the time it takes them to complete the homework. Example 8.3 Brenda claims that newly graduated data science majors from CSU have an average salary of $63,000, but Rawand believes it is smaller. Example 8.4 Rosa believes less than 50% of college students have consumed alcohol. You collect a random sample of 20 students to test the claim. Different Types of Hypothesis Tests There are three different types of hypothesis tests. 1. Left-tailed test 2. Right-tailed test 3. Two-tailed test Errors in Hypothesis Testing After collecting a sample, we will decide which hypothesis is supported by the data. Since we don’t know the true parameter value, there is a chance we will make an error. Two Types of Error Type I error (\\(\\alpha\\)): Reject \\(H_0\\) when \\(H_0\\) is true. Type II error (\\(\\beta\\)): Fail to reject \\(H_0\\) when \\(H_0\\) is false. Note: \\(\\alpha\\) is also called the level of significance for a test. Example 8.5 Jury trials, \\(H_0:\\) Defendant is innocent vs. \\(H_A:\\) Defendant is guilty Six Steps for Hypothesis Testing Different textbooks will give a different number of steps or outline on how to complete a hypothesis test. For our class, stick to the following method. Step 1: State the hypotheses. Step 2: Determine the level of significance. Step 3: Compute a test statistic. Step 4: Calculate a p-value and plot the results. The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one obtained from the sample data, assuming the null hypothesis is true. The p-value does not measure the probability that the null hypothesis is true. ASA Statement on p-values: https://www.amstat.org/asa/files/pdfs/p-valuestatement.pdf Step 5: Make a statistical decision. Step 6: Interpret the statistical decision (in the context of the problem). 8.2 Tests About a Population Mean, \\(\\mu\\) Case 1: \\(\\sigma\\) known If \\(\\sigma\\) is known, then the test statistic is the following: \\[z = \\frac{\\bar{x}-\\mu_0}{\\sigma/\\sqrt{n}}\\] Example 8.6 A materials scientist is developing a new alloy intended for use in aerospace engineering, where the average tensile strength is claimed to be 1500 MPa (megapascals). Based on previous research, the tensile strength of this alloy is known to be normally distributed with a population variance of 40 MPa². To validate the claim, the scientist tests a sample of 10 specimens and finds a sample mean tensile strength of 1495 MPa. The goal is to perform a hypothesis test at a significance level of \\(\\alpha = 0.05\\) to determine whether the average tensile strength is actually different from the claimed 1500 MPa. Example 8.7 An engineering firm claims that a new fuel-efficient engine design provides an average fuel efficiency of at least 40 miles per gallon (mpg). The design specifications guarantee this minimum efficiency, and engineers want to verify that the engine consistently meets this standard. It is known from previous tests that fuel efficiency for this type of engine follows a normal distribution with a population variance of 9 mpg². A random sample of 8 engines is tested, and the sample shows an average fuel efficiency of 38.5 mpg. The engineers decide to conduct a one-sided hypothesis test at a significance level of \\(\\alpha = 0.05\\) to determine if the true average fuel efficiency is actually lower than the claimed 40 mpg. In addition, state what Type I and Type II errors are in the context of the problem. Case 2: \\(\\sigma\\) unknown If \\(\\sigma\\) is unknown, then the test statistic is the following: \\[t_{stat} = \\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}, df=n-1\\] Example 8.8 A chemist claims that a newly synthesized chemical compound has an average melting point of 250°C. However, the chemist does not know the population variance for the melting point of this compound. To test the consistency of the compound, a sample of 31 batches is produced and their melting points are measured. The sample has a mean melting point of 247°C, with a sample standard deviation of 8.4°C. The chemist wants to perform a hypothesis test at a significance level of \\(\\alpha = 0.10\\) to determine whether the true average melting point is different from the claimed 250°C. Example 8.9 A software engineer claims that a new web application backend has an average response time of less than 200 milliseconds under typical user load. However, the team wants to verify this claim to ensure that the system meets performance expectations. To test this, a sample of 50 response times is collected from the application under standard load conditions. The sample mean response time is 210 milliseconds, with a sample standard deviation of 25 milliseconds. The engineer performs a one-sided hypothesis test at a significance level of \\(\\alpha = 0.05\\) to determine if the true average response time is actually greater than the claimed 200 milliseconds. 8.3 Tests Concerning a Population Proportion, \\(p\\) We can run a hypothesis test for a population proportion using the following formula: \\[z = \\frac{\\bar{p}-p_0}{\\sqrt{p_0(1-p_0)/n}}\\] Example 8.10 A mathematics department claims that at least 80% of its students pass a standardized math proficiency exam required for graduation. A new curriculum is introduced, and the department wants to evaluate whether the pass rate has fallen below this benchmark. To test this, a random sample of 150 students who took the exam under the new curriculum is selected, and 112 students passed. The department conducts a one-sided hypothesis test at a significance level of \\(\\alpha = 0.05\\) to determine if the true pass rate is now less than the claimed 80%. 8.4 R Companion for Chapter 8 Example 8.11 The faithful dataset of Old Faith Geyser data is used again for this example. Since the population variance of Old Faithful eruption duration is unknown, the t-distribution is used to complete the following hypothesis tests. Let \\(\\mu\\) denote the true mean eruption duration of Old Faithful and we want to test \\(H_0: \\mu = 3\\) vs. \\(H_a: \\mu \\neq 3\\) at \\(\\alpha = 0.05\\). t.test(faithful$eruptions,mu = 3) ## ## One Sample t-test ## ## data: faithful$eruptions ## t = 7.0483, df = 271, p-value = 1.502e-11 ## alternative hypothesis: true mean is not equal to 3 ## 95 percent confidence interval: ## 3.351534 3.624032 ## sample estimates: ## mean of x ## 3.487783 From the output, we see that the p-value is \\(1.5 \\cdot 10^{-11}\\). This is less than \\(\\alpha=0.05\\), so we would reject \\(H_0: \\mu = 3\\) and conclude the true mean eruption duration of Old Faithful differs from 3 minutes. Suppose we want to test \\(H_0: \\mu = 3.5\\) vs. \\(H_a: \\mu &lt; 3.5\\) at \\(\\alpha=0.05\\). In other words, we would like to test to see if the mean is less than 3.5 minutes. t.test(faithful$eruptions,mu=3.5,alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: faithful$eruptions ## t = -0.17653, df = 271, p-value = 0.43 ## alternative hypothesis: true mean is less than 3.5 ## 95 percent confidence interval: ## -Inf 3.602007 ## sample estimates: ## mean of x ## 3.487783 Here, the p-value is 0.43, so we would fail to reject \\(H_0: \\mu = 3.5\\). In other words, we don’t have sufficient evidence to conclude the true mean eruption duration is less than 3.5 minutes. "],["inference-based-on-two-samples.html", "Chapter 9 Inference Based On Two Samples 9.1 z-tests and Confidence Intervals for a Difference Between Two Population Parameters 9.2 The Two-Sample t-test and Confidence Interval 9.3 Analysis of Paired Data 9.4 Inferences Concerning a Difference Between Population Proportions 9.5 Statistical Power 9.6 R Companion for Chapter 9", " Chapter 9 Inference Based On Two Samples In Chapters 7 and 8, we explored hypothesis tests and confidence intervals for making statistical inferences about a single population parameter. In this chapter, we will expand these methods to compare two population parameters. We’ll cover hypothesis tests for both independent and paired samples using z-tests and t-tests, as well as tests for comparing two population proportions. Additionally, we will introduce confidence intervals for the difference between two parameters and discuss the concept of statistical power in the context of two-sample inference. 9.1 z-tests and Confidence Intervals for a Difference Between Two Population Parameters Example 9.1 A manufacturer produces two types of industrial batteries, Model A and Model B, which are used in renewable energy systems. The company claims that the lifespan of both battery models is approximately the same, but an engineering team wants to verify this by conducting a study. They know the population standard deviations of the lifespans for both models from previous extensive testing. A sample of 15 Model A batteries has an average lifespan of 8.5 years with a known population standard deviation of 1.2 years. A sample of 15 Model B batteries has an average lifespan of 9.1 years with a known population standard deviation of 1.5 years. The engineers perform a hypothesis test to determine if there is sufficient evidence to suggest a difference in the average lifespan between the two battery models. One way to test this claim is create a confidence interval for the difference in means (\\(\\mu_1 - \\mu_2\\)). Another approach to testing this claim is to see how consistent that sample is with initial hypothesis of \\(\\mu_1-\\mu_2=0\\). Note: We must make an assumption of whether the samples from the two populations are independent or dependent. In this section (9.1) and the next section (9.2), we will assume the samples are independent. Assumptions for Comparison of Independent Samples \\(X_1, X_2, \\ldots, X_{n_1}\\) are a random sample from a distribution with mean \\(\\mu_1\\) and variance \\(\\sigma_1^2\\). \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) are a random sample from a distribution with mean \\(\\mu_2\\) and variance \\(\\sigma_2^2\\). The X and Y samples are independent. Notation Theorem 9.1 If X and Y are independent, then \\(E[\\bar{X}-\\bar{Y}] = \\mu_1 - \\mu_2\\) \\(Var(\\bar{X} - \\bar{Y}) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\) (\\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) may be unknown) Proof: Writing Hypotheses for Difference in Means Testing \\(\\mu_1 \\neq \\mu_2\\) Testing \\(\\mu_1 &gt; \\mu_2\\) Testing \\(\\mu_1 &lt; \\mu_2\\) We can complete a hypothesis test for the difference in means (assuming the variances or standard deviations of the two populations are known) by using the following test statistic. \\[z= \\frac{(\\bar{x_1}-\\bar{x_2})-D_0)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\] Example 9.2 A sample of 15 Model A batteries has an average lifespan of 8.5 years with a known population standard deviation of 1.2 years. A sample of 15 Model B batteries has an average lifespan of 9.1 years with a known population standard deviation of 1.5 years. The engineers perform a hypothesis test to determine if there is sufficient evidence to suggest a difference in the average lifespan between the two battery models. A two-sided confidence interval for the difference in means can also be completed with the following formula, assuming again that the population standard deviations are known. \\[(\\bar{x}_1-\\bar{x}_2) \\pm z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\\] 9.2 The Two-Sample t-test and Confidence Interval If the population standard deviations of the two populations are unknown, we can estimate them and use similar methods as in section 9.1. A confidence interval for the difference in means can be completed with the following formula if the population standard deviations are unknown. \\[(\\bar{x}_1-\\bar{x}_2) \\pm t_{\\alpha/2,\\nu} \\cdot \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\] For the above confidence interval, you must calculate the degrees of freedom (\\(\\nu\\)). Round the value of \\(\\nu\\) to the next lowest integer if it is not a whole number. \\[\\nu = \\frac{\\big(s_1^2/n_1 + s_2^2/n_2\\big)^2}{(s_1^2/n_1)^2/(n_1-1) + (s_2^2/n_2)^2/(n_2-1)}\\] Example 9.3 It has been claimed that people in Okinawa live longer on average than people on the Japanese mainland. To investigate this, a study is conducted. A random sample of 25 people from Okinawa shows an average lifespan of 84.7 years, with a sample variance of 25 years². Meanwhile, a sample of 22 people from the Japanese mainland finds an average lifespan of 83.2 years, with a sample variance of 20 years². We will use this data to calculate a 95% confidence interval for the difference in average lifespan between Okinawa and the Japanese mainland. Given the sample sizes and variances, the degrees of freedom for the test can be approximated as 43. The goal is to determine whether there is a statistically significant difference in the average lifespan between the two populations. We can complete a hypothesis test for the difference in means (assuming the variances or standard deviations of the two populations are unknown) by using the following test statistic. This is called a two-sample t-test. \\[t= \\frac{(\\bar{x}_1-\\bar{x}_2-D_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}, df = \\nu \\] \\[\\nu = \\frac{\\big(s_1^2/n_1 + s_2^2/n_2\\big)^2}{(s_1^2/n_1)^2/(n_1-1) + (s_2^2/n_2)^2/(n_2-1)}\\] Example 9.4 Using the data from the Okinawa life expectancy study, conduct a hypothesis test at a significance level of \\(\\alpha = 0.05\\) to determine if the average lifespan in Okinawa (\\(\\mu_1\\)) is significantly greater than the average lifespan on the Japanese mainland (\\(\\mu_2\\)). 9.3 Analysis of Paired Data In the previous two sections, we assumed that the two samples were independent. However, this assumption does not always hold true in practice. Example 9.5 Examples of paired samples include: Before/after studies: The same individuals are measured before and after an intervention. Matched-pair designs: For example, comparing blood pressure in the right and left arms of the same individuals. Sibling studies: Measuring certain traits (e.g., height or test scores) in siblings to see if there are significant differences. Repeated measurements: Taking the same measurement under different conditions (e.g., testing reaction time under different levels of noise for the same participants). Definition 9.1 A sample is paired if the same subjects or closely matched subjects are measured twice under different conditions. A paired t-test helps control for differences between individuals by focusing on the differences between the paired measurements. Advantage: A paired t-test allows us to use a smaller sample size because it reduces the variance that comes from differences between individuals, leaving only the variation between the paired measurements themselves. Disadvantage: It’s not always feasible to collect paired data. If this is the case, you can use a two-sample t-test under the assumption that the samples are independent. CI and Hypothesis Tests for Paired Samples (\\(\\mu_d\\)) When doing inference for the true average change (\\(\\mu_d\\)), we compare ``treatments” (e.g. before/after) on an individual level. Create a ``new”, single sample based on the sample differences: Using the ``new’’ sample, use the same procedures for inference as for a single population. \\[t = \\frac{\\bar{d}-\\mu_{d_0}}{s_d/\\sqrt{n}}, df = n-1\\] \\[\\bar{d} \\pm t_{\\alpha/2,n-1} \\cdot \\frac{s_d}{\\sqrt{n}}\\] Example 9.6 A pharmaceutical company is testing a new drug designed to lower blood pressure. They select a group of 10 patients with high blood pressure and measure their systolic blood pressure before and after administering the drug. The company wants to know if the drug significantly lowers blood pressure. Assume that the differences in blood pressure before and after the treatment are approximately normally distributed. Complete a six-step hypothesis test to test the claim the average systolic blood pressure is less after receiving the drug at \\(\\alpha=0.05\\). 9.4 Inferences Concerning a Difference Between Population Proportions We can complete a hypothesis test for the difference in proportions. \\[z = \\frac{(\\bar{p_1}-\\bar{p_2})-D_0}{\\sqrt{(\\hat{p}\\cdot (1-\\hat{p})\\cdot(\\frac{1}{n_1} + \\frac{1}{n_2})}}\\] \\[\\hat{p} = \\frac{n_1}{n_1+n_2} \\bar{p_1} + \\frac{n_2}{n_1+n_2} \\bar{p_2}\\] We can also create a confidence interval for the difference in proportions. \\[(\\bar{p_1}-\\bar{p_2}) \\pm z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\bar{p_1}(1-\\bar{p_1})}{n_1} + \\frac{\\bar{p_2}(1-\\bar{p_2})}{n_2}}\\] Example 9.7 A study is conducted to compare the proportion of college students who binge drank in the last year at two campuses: CSU-Fort Collins and CSU-Pueblo. A random sample of 80 students from CSU-Fort Collins found that 44 of them reported binge drinking in the past year. Meanwhile, a sample of 75 students from CSU-Pueblo found that 45 of them reported binge drinking. Create a 90% confidence interval for the difference in proportions of binge drinking between the two campuses. Additionally, test whether there is significant evidence to suggest that one campus has a higher proportion of binge drinkers than the other. 9.5 Statistical Power Statistical power is an essential concept in hypothesis testing, representing the probability that a test correctly rejects a false null hypothesis. In other words, it quantifies how likely we are to detect an effect when there is one. Definitions Type I error (\\(\\alpha\\)): The probability of rejecting the null hypothesis when it is actually true. For instance, if \\(\\alpha = 0.05\\), there is a 5% chance of incorrectly rejecting the null hypothesis when it is true. Type II error (\\(\\beta\\)): The probability of failing to reject the null hypothesis when it is false. Statistical Power: Defined as the probability of rejecting the null hypothesis when it is false, i.e., correctly detecting a true effect. \\[ \\text{Power} = 1 - \\beta = P(\\text{Reject } H_0 \\&gt;|\\&gt; H_0 \\text{ is false}) \\] Power depends on several factors: The effect size (how large the true difference is between the groups). The sample size (larger sample sizes increase power). The significance level (\\(\\alpha\\)). The variability in the data (lower variability increases power). Example 9.8 Consider a study to test the effectiveness of a new online learning platform for improving exam scores. The platform’s developers claim that it will increase the average exam score by 5 points compared to traditional learning methods. Previous studies show that the standard deviation of exam scores is 10 points. You decide to conduct a study with 50 students in each group (online and traditional learning). Null Hypothesis (\\(H_0\\)): The mean exam score is the same for both groups. Alternative Hypothesis (\\(H_a\\)): The mean exam score for the online group is higher by 5 points. We want to know the statistical power of detecting this difference if it exists, using an online power calculator instead of hand calculations. Step-by-Step Power Analysis Determine effect size: Using Cohen’s \\(d\\), the effect size can be calculated as: \\[ d = \\frac{\\mu_1 - \\mu_2}{\\sigma} = \\frac{5}{10} = 0.5 \\] This represents a moderate effect size. Choose significance level (\\(\\alpha\\)): Set \\(\\alpha = 0.05\\) for a 5% risk of Type I error. Calculate Power in R: Rather than performing hand calculations, you can input these parameters into R. Simply provide the effect size, sample size, significance level, and test type (two-sample t-test). Interpret the results: R will output the statistical power. For this example, the power may be around 0.75, meaning there is a 75% chance of detecting a 5-point difference in exam scores if the platform truly has this effect. Cohen’s \\(d\\) and Effect Size Guidelines Small effect size: \\(d &lt; 0.2\\) Moderate effect size: \\(0.2 &lt; d &lt; 0.8\\) Large effect size: \\(d &gt; 0.8\\) Key Takeaways Larger sample sizes increase statistical power, as they provide more data to detect true differences. Larger effect sizes (i.e., bigger differences between the groups) also increase power. Smaller variability (lower standard deviation) enhances the power of a test by making the effect easier to detect. Example 9.9 A tech company has released a new version of their app and wants to compare user engagement time (in minutes) between the new version and the old version. Suppose the standard deviation of engagement time is 20 minutes, and they expect the new version to increase engagement by 10 minutes. They plan to randomly select 100 users for each version. Null Hypothesis (\\(H_0\\)): There is no difference in engagement time between the two versions. Alternative Hypothesis (\\(H_a\\)): Engagement time is higher for the new version by 5 minutes. Using R, determine the probability of detecting this 5-minute increase with 100 users per group at \\(\\alpha = 0.05\\), that is, calculate the statistical power. library(pwr) # Parameters effect_size = 0.25 # Given effect size n1 = 100 # Sample size for Group 1 n2 = 100 # Sample size for Group 2 alpha = 0.05 # Significance level # Power calculation pwr.t2n.test(n1 = n1, n2 = n2, d = effect_size, sig.level = alpha, alternative = &quot;greater&quot;) ## ## t test power calculation ## ## n1 = 100 ## n2 = 100 ## d = 0.25 ## sig.level = 0.05 ## power = 0.5465182 ## alternative = greater 9.6 R Companion for Chapter 9 Example 9.10 The dataset is included in the base version of R. This dataset include 71 observations of the growth rate of chickens using a variety of feed supplements. data(chickwts) # pick out some random indices of the dataset and look at the elements set.seed(2020) chickwts[sample(x=71,size=10,replace = F),] ## weight feed ## 28 250 soybean ## 22 271 linseed ## 65 318 casein ## 17 148 linseed ## 36 248 soybean ## 42 226 sunflower ## 49 325 meatmeal ## 45 334 sunflower ## 56 242 meatmeal ## 8 124 horsebean We can create a boxplot of the weights for the various types of feed. boxplot(weight~feed,data=chickwts) Based on the boxplot, there appears be a difference in weights between casein and horsebean. To test this, let’s run the hypothesis test \\(H_0: \\mu_1 - \\mu_2 = 0\\) vs. \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\), where \\(\\mu_1\\) is the true mean weight of chicks given casein and \\(\\mu_2\\) is the true mean weight of chicks given horsebean. We will use \\(\\alpha = 0.05\\) for the following examples. # create new dataset of chicks given casein chicks1 = chickwts[chickwts$feed==&quot;casein&quot;,] chicks2 = chickwts[chickwts$feed==&quot;horsebean&quot;,] t.test(chicks1$weight,chicks2$weight,mu=0) ## ## Welch Two Sample t-test ## ## data: chicks1$weight and chicks2$weight ## t = 7.3423, df = 18.36, p-value = 7.21e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 116.6982 210.0685 ## sample estimates: ## mean of x mean of y ## 323.5833 160.2000 The test statistic is \\(t=7.34\\) and corresponds to p-value of \\(7.21 \\cdot 10^{-7}\\). Since the p-value is less than \\(\\alpha=0.05\\), we would reject \\(H_0\\) and conclude that the true mean chick weights differ between casein and horsebean feed. Example 9.11 Suppose we want to test if the true mean weight of casein feed chicks is more than 200 units greater than the true mean weight of horsebean feed chicks, i.e., \\(H_0: \\mu_1 - \\mu_2 = 200\\) vs. \\(H_a: \\mu_1 - \\mu_2 &gt; 200\\). t.test(chicks1$weight,chicks2$weight,mu=200,alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: chicks1$weight and chicks2$weight ## t = -1.6455, df = 18.36, p-value = 0.9416 ## alternative hypothesis: true difference in means is greater than 200 ## 95 percent confidence interval: ## 124.8371 Inf ## sample estimates: ## mean of x mean of y ## 323.5833 160.2000 The test statistic for this hypothesis test is \\(t=-1.6455\\) with a p-value of \\(0.9416\\). At \\(\\alpha=0.05\\), we would fail to reject \\(H_0: \\mu_1 - \\mu_2 = 200\\) and conclude there is not sufficient evidence to say that the true mean weight of casein feed chicks is more than 200 units greater than the true mean weight of horsebean feed chicks. "],["analysis-of-variance-anova.html", "Chapter 10 Analysis of Variance (ANOVA) 10.1 Single-Factor ANOVA 10.2 Multiple Comparison in ANOVA 10.3 R Companion for Chapter 10", " Chapter 10 Analysis of Variance (ANOVA) In Chapter 9, we compared population means and proportions between two groups. Here, we extend this analysis to Analysis of Variance (ANOVA), a method for comparing the means across three or more populations. ANOVA helps us determine if observed differences among group means are statistically significant or due to chance, making it a valuable tool for analyzing differences across multiple groups in various fields. 10.1 Single-Factor ANOVA Illustration of ANOVA Motivation: Q: Are the means the same in each population? Does \\(\\mu_1 = \\mu_2 = \\ldots = \\mu_k\\)? A: Collect a sample from each population and use ANOVA to make the determination. Our hypotheses will be the following: \\(H_0:\\mu_1 = \\mu_2 = \\ldots = \\mu_k\\) \\(H_a:\\text{ at least one } \\mu_i \\text{ differs}\\) Recall that a two-sample t-test will compare the means of two populations (or treatments). ANOVA extends this to 3 or more populations (or treatments). Example 10.1 Do the average GPAs (grade point averages) differ by class level at this university? A small pilot study looked into this possibility by collecting GPAs from five randomly selected students from each of four levels (first-year through fourth-year). Year Sample Size Sample Mean Sample Std Dev Freshman 5 2.3 0.318 Sophomore 5 3.3 0.412 Junior 5 3.0 0.424 Senior 5 3.3 0.453 Is there enough evidence to suggest that the average GPA is not the same for all four levels? Our hypotheses for this example will be: Definition 10.1 A factor is a characteristic that distinguishes between populations or treatments. Definition 10.2 A level of a factor is one specific population or treatment within that factor. Example 10.2 We want to test the average effectiveness of four different drugs in treating a specific condition. Factor: Levels: Example 10.3 We want to compare the average growth of sunflowers for six growing conditions. Factor: Levels: We will measure a quantitative response (Y) for a sample from each level of the factor. Often, ANOVA data come from a controlled experiment, where \\(k\\) represents the number of levels, treatments, or populations. Notation: \\[N = n_1 + n_2 + \\ldots + n_k\\] \\[T = n_1 \\bar{x}_1 + n_2 \\bar{x}_2 + \\ldots + n_k \\bar{x}_2 \\] \\[\\bar{\\bar{x}}= T/N\\] Definition 10.3 Sum of Squares (SS) quantifies two types of variation in data. Treatment Sum of Squares (SSTR) - measures the variability between groups. How far apart are the sample means of the k populations? \\[SSTR = n_1(\\bar{x}_1 - \\bar{\\bar{x}})^2 + n_2(\\bar{x}_2 - \\bar{\\bar{x}})^2 + \\ldots + n_k(\\bar{x}_k - \\bar{\\bar{x}})^2\\] \\[df_{TR} = k-1\\] Error Sum of Squares (SSE) - measures the variability within the k populations. How spread out are the individual populations? \\[SSE = (n_1-1)s_1^2 + (n_2-1)s_2^2 + \\ldots (n_k-1)s_k^2\\] \\[df_{E} = N-k\\] **Total Sum of Squares (SST)} - measures the total variability in the data, ignoring populations and treatments. \\[SST = SSTR + SSE = \\sum (x-\\bar{\\bar{x}})^2\\] \\[df_T = N-1\\] Example 10.4 Calculate SSTR, SSE, and SST (and the associated degrees of freedom) for GPA data set. Mean Squares Definition 10.4 In ANOVA, a mean square (MS) represents an average measure of variation. It is obtained by dividing a sum of squares (SS) by its associated degrees of freedom (df). Each mean square provides a way to quantify the average variability within or between groups, allowing us to make comparisons across different sources of variation in the data. \\[MSTR = \\frac{SSTR}{k-1}\\] \\[MSE = \\frac{SSE}{N-k}\\] An ANOVA F-statistic can be calculated from the mean squares as follows: \\[F_{test} = \\frac{MSTR}{MSE}, df_1 = k, df_2 = N-k\\] Some notes: Under the null hypothesis \\(H_0\\) (\\(\\mu_1 = \\mu_2 = \\ldots \\mu_k\\)), \\(MSTR \\approx MSE\\). If all population means are equal, \\(F_{test} \\approx 1\\). If one or more of the populatino means are not equal, \\(F_{test} &gt;&gt; 1\\). \\(F_{test}\\) follows an F-distribution with parameters \\(df_1 = k\\), \\(df_2 = N-k\\) Example 10.5 Calculate MSTR, MSE, \\(F_{test}\\), and the p-value for the GPA data set. ANOVA Assumptions ANOVA assumes that population or treatment distributions are normal and have equal variances. While these assumptions can be checked in R, we will generally assume they hold for the examples analyzed here. \\(\\chi^2\\) distribution F distribution One Way ANOVA F-test Step 1: State the hypotheses. Step 2: State the level of significance. Step 3: Calculate the test statistic. Step 4: Calculate the p-value and plot. Step 5: Make a statistical decision. Step 6: Interpret your decision in the context of the problem. ANOVA table An ANOVA table summarizes the sources of variability in data by dividing it into “between-group” and “within-group” components. It includes values for sum of squares (SS), degrees of freedom (df), mean square (MS), F-statistic, and p-value. Using R to complete ANOVA Since many of these calculations can be tedious to calculate, a statistical program like R can be used to complete the analysis. # GPA data set fresh.gpa = c(2.05, 2.20, 2.00, 2.50, 2.75) soph.gpa = c(3.00, 2.80, 3.80, 3.60, 3.30) junior.gpa = c(3.30, 3.20, 2.30, 2.90, 3.30) senior.gpa = c(3.50, 3.10, 2.90, 3.00, 4.00) # Make one list of all GPAs gpa = c(fresh.gpa,soph.gpa,junior.gpa,senior.gpa) # Create labels year = c(rep(&quot;fresh&quot;,5),rep(&quot;soph&quot;,5),rep(&quot;junior&quot;,5),rep(&quot;senior&quot;,5)) # calculate ANOVA fit = aov(gpa~year) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## year 3 3.338 1.1125 6.781 0.00367 ** ## Residuals 16 2.625 0.1641 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Example 10.6 Using the R output from the previous page, complete a six-step hypothesis test to whether the mean GPAs at the four college levels are the same at \\(\\alpha=0.05\\). 10.2 Multiple Comparison in ANOVA Suppose that we conclude that the means of the k populations are not all equal. Which populations means do differ? Q: Can we just do two-sample t-tests for all pairs of populations? A: Yes and no. We need to make an adjustment for simultaneously completing a group of tests. Example 10.7 In the GPA example, we had four populations. Suppose the null hypothesis is true (\\(\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\\)) and \\(\\alpha=0.05\\). Tukey’s Procedure Tukey’s Procedure (Tukey’s Honest Significant Difference or Tukey HSD) is a multiple comparisons procedure to account for completing a number of simultaneous tests. Confidence intervals for the difference in means (\\(\\mu_i - \\mu_j\\)) can be computed using Tukey’s Procedure. These calculations utilize another continuous distribution, the studentized range distribution (Q). While these calculations can be completed by hand, we’ll focus on calculating them in R. Example 10.8 Returning to the GPA data set, determine which population means differ at \\(\\alpha=0.05\\) using a Tukey adjustment. TukeyHSD(fit) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = gpa ~ year) ## ## $year ## diff lwr upr p adj ## junior-fresh 0.7 -0.03291823 1.4329182 0.0638299 ## senior-fresh 1.0 0.26708177 1.7329182 0.0062342 ## soph-fresh 1.0 0.26708177 1.7329182 0.0062342 ## senior-junior 0.3 -0.43291823 1.0329182 0.6527735 ## soph-junior 0.3 -0.43291823 1.0329182 0.6527735 ## soph-senior 0.0 -0.73291823 0.7329182 1.0000000 # options for clean y-axis labels par(las = 1) par(mar = c(5, 8, 4, 2)) plot(TukeyHSD(fit)) 10.3 R Companion for Chapter 10 Example 10.9 This example uses the dataset again. This dataset include 71 observations of the growth rate of chickens using a variety of feed supplements. The boxplot is produced again below. data(chickwts) boxplot(weight~feed,data=chickwts) Using the theory from Chapter 10, we are ready to test if there is evidence of a difference in mean weights of chicks for the six types of feed. Our ANOVA F-test hypotheses will be:\\ \\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 = \\mu_6\\) vs. $H_a: $ at least one \\(\\mu_i\\) differs (\\(i=1,2,3,4,5,6\\)).\\ Let’s run this F-test at \\(\\alpha = 0.01\\). fit = aov(weight~feed,data=chickwts) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## feed 5 231129 46226 15.37 5.94e-10 *** ## Residuals 65 195556 3009 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The resulting F-test statistic is \\(F_{test} = 15.37\\) (\\(df_1 = 5, df_2 = 65\\)) and a p-value of \\(5.94 \\cdot 10^{-10}\\). At a significance level of \\(\\alpha = 0.01\\), we would reject \\(H_0\\) and conclude that at least one of the true mean weights differs by diet. Which \\(\\mu_i\\) differ? We can answer this by running all pairwise comparisons of \\(\\mu_i\\) and \\(\\mu_j\\) (\\(i \\neq j\\)) and making a multiple comparisons adjustment. Let’s use a family-wise error rate of \\(\\alpha = 0.01\\). TukeyHSD(fit,conf.level = 0.99) ## Tukey multiple comparisons of means ## 99% family-wise confidence level ## ## Fit: aov(formula = weight ~ feed, data = chickwts) ## ## $feed ## diff lwr upr p adj ## horsebean-casein -163.383333 -245.964363 -80.802304 0.0000000 ## linseed-casein -104.833333 -183.571256 -26.095411 0.0002100 ## meatmeal-casein -46.674242 -127.181777 33.833292 0.3324584 ## soybean-casein -77.154762 -153.028522 -1.281001 0.0083653 ## sunflower-casein 5.333333 -73.404589 84.071256 0.9998902 ## linseed-horsebean 58.550000 -24.031030 141.131030 0.1413329 ## meatmeal-horsebean 116.709091 32.439113 200.979069 0.0001062 ## soybean-horsebean 86.228571 6.373743 166.083400 0.0042167 ## sunflower-horsebean 168.716667 86.135637 251.297696 0.0000000 ## meatmeal-linseed 58.159091 -22.348444 138.666626 0.1276965 ## soybean-linseed 27.678571 -48.195189 103.552332 0.7932853 ## sunflower-linseed 110.166667 31.428744 188.904589 0.0000884 ## soybean-meatmeal -30.480519 -108.189144 47.228105 0.7391356 ## sunflower-meatmeal 52.007576 -28.499959 132.515111 0.2206962 ## sunflower-soybean 82.488095 6.614335 158.361856 0.0038845 There are several statistically significant differences in mean chick weights at \\(\\alpha=0.01\\). Casein differs from horsebean, linseed, and soybean. Horsebean differs from casein, meatmeal, soybean, and sunflower. Linseed differs from casein and sunflower. Meatmeal differs from horsebean. Sunflower differs from horsebean, linseed, and soybean. Soybean differs from sunflower, horsebean, and casein. plot(TukeyHSD(fit,conf.level = 0.99)) "],["simple-linear-regression.html", "Chapter 11 Simple Linear Regression 11.1 The Simple Linear Regression Model 11.2 Correlation 11.3 Estimating Model Parameters 11.4 Assessing Model Fit and Inferences for the Slope Parameter, \\(\\beta_1\\) 11.5 The Prediction of Future Y Values 11.6 Model Diagnostics 11.7 Extrapolation 11.8 R Companion for Chapter 11", " Chapter 11 Simple Linear Regression In Chapter 10, we explored ANOVA, a method for examining the relationship between a categorical variable (with three or more groups or treatments) and a quantitative outcome. In this chapter, we shift to simple linear regression, which analyzes the relationship between two quantitative variables. Linear regression allows us to model and predict how changes in one variable are associated with changes in another, providing insights into trends, correlations, and potential causation in data. 11.1 The Simple Linear Regression Model In many cases, we wish to quantify and analyze the relationship between two quantitative variables, where one variable helps predict or explain the other. Example 11.1 The following data give the speed of cars (mph) and the distances (feet) taken to stop. Note that the data were recorded in the 1920s. data(cars) plot(cars,pch=20) Variables: X = independent variable / predictor variable / explanatory variable Y = dependent variable / response variable In simple linear regression, we examine the relationship between a single predictor variable and a single response variable. When multiple predictor variables are involved, we use multiple linear regression. Goal: Using a simple random sample, we aim to build a model that represents the relationship between \\(X\\) and \\(Y\\) for the population. This model is represented by the least squares regression line – the line that best fits the data points, minimizing the sum of squared differences between observed values and the predictions made by the line. Estimated Regression Equation: Population Model: Assumptions and Notes for Simple Linear Regression (SLR): Linearity in the Parameters: The relationship between \\(X\\) and \\(Y\\) can be described using linear parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the equation \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\). Error Assumptions: The errors (\\(\\epsilon_i\\)) are independent, identically distributed normal random variables with mean 0 and constant variance \\(\\sigma^2\\). One objective of SLR is to estimate \\(\\sigma^2\\), the variance of these errors. Expected Value of \\(Y\\): For a given value of \\(X\\), the mean of \\(Y\\) is represented by the line of best fit: \\[E[Y|X] = \\beta_0 + \\beta_1 X \\] Estimating \\(\\sigma\\): Because observed data points do not lie exactly on the line of best fit, we estimate \\(\\sigma\\) to quantify the spread or variability of data around this line. 11.2 Correlation Recall: Correlation is a measure of linear relationship between two quantitative variables. The population correlation coefficient is represented by \\(\\rho\\) and the sample correlation coefficient is represented by \\(r\\). Correlation is related to covariance, \\(\\rho = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\), but it is independent of units and is bounded by \\(\\pm 1\\), i.e., \\(-1 \\leq \\rho \\leq 1\\). If two variables are uncorrelated, then they are not necessarily independent. This could be the case if the variables have a nonlinear relationship. \\[r = \\frac{S_{xy}}{\\sqrt{S_{xx}}\\sqrt{S_{yy}}}, \\text{where } S_{xx} = \\sum_{i=1}^n (x_i-\\bar{x})^2, S_{yy} = \\sum_{i=1}^n (y_i-\\bar{y})^2, S_{xy} = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\] Example 11.2 Some examples of some scatterplots and their associated sample correlation coefficients. Note: Correlation does not imply causation! 11.3 Estimating Model Parameters Criterion for determining the line of best fit We want to minimize the sum of squared errors (SSE), that is, we want to find \\(b_0\\) and \\(b_1\\) that minimize \\(SSE = \\sum_{i=1}^n [y_i - (b_0 + b_1 x)]^2\\). Illustration: Normal Equations: SLR estimates \\[\\text{Estimated Regression Equation: } \\hat{y} = b_0 + b_1 x\\] \\[b_0 = \\bar{y} - b_1 \\bar{x}\\] \\[b_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{s_y}{s_x} \\cdot r_{xy} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\] \\[S_{xx} = \\sum_{i=1}^n (x_i-\\bar{x})^2 \\qquad S_{yy} = \\sum_{i=1}^n (y_i-\\bar{y})^2 \\qquad S_{xy} = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\] Interpreting \\(b_1\\) and \\(b_0\\) The estimated slope, \\(b_1\\), can be interpreted as the estimated change in y for a unit increase in x. The estimated intercept, \\(b_0\\), can be interpreted as the estimated value of y when x is 0. Example 11.3 Fill in the table to calculate the estimated regression equation and plot it on the figure below. \\(\\Sigma\\) \\(x_i\\) 1 2 3 4 5 \\(y_i\\) 3 5.5 5.5 6.5 9.5 \\(x_i - \\bar{x}\\) \\((x_i - \\bar{x})^2\\) \\(y_i - \\bar{y}\\) \\((x_i - \\bar{x})(y_i - \\bar{y})\\) Example 11.4 Here’s how to use R to do the calculations. # enter the data x = c(1,2,3,4,5) y = c(3,5.5,5.5,6.5,9.5) # run the linear regression and see the results model = lm(y~x) summary(model) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## 1 2 3 4 5 ## -0.2 0.9 -0.5 -0.9 0.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.8000 0.9381 1.919 0.1508 ## x 1.4000 0.2828 4.950 0.0158 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8944 on 3 degrees of freedom ## Multiple R-squared: 0.8909, Adjusted R-squared: 0.8545 ## F-statistic: 24.5 on 1 and 3 DF, p-value: 0.01582 Example 11.5 Here is the car stopping distance example given at the beginning of the chapter. Note that we will load the ``cars’’ dataset that is provided in R. # load the data data(cars) # run the regression and summarize the results model = lm(dist~speed,data=cars) summary(model) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 # plot the data and the line of best fit plot(dist~speed,data=cars,pch=20) abline(model) 11.4 Assessing Model Fit and Inferences for the Slope Parameter, \\(\\beta_1\\) While an estimated regression equation can be calculated for any set of data pairs, it is essential to determine whether a linear model is appropriate for the data. There are multiple ways to assess model fit. Coefficient of determination, \\(R^2\\) Linear Regression Hypothesis Test for \\(b_1\\) Model diagnostics (more on this later) ANOVA for Regression Often, we wish to analyze the quality of the estimated regression equation line using an analysis of variance approach. In this setup, the total variation of the dependent variable is partitioned into subcomponents. \\(\\boxed{SST = \\sum_{i=1}^n (y_i -\\bar{y})^2}\\) = total sum of squares = ``total variation’’ \\(\\boxed{df_T = n-1}\\) (n = sample size) \\(\\boxed{SSR = \\sum_{i=1}^n (\\hat{y_i} - \\bar{y})^2}\\) = regression sum of squares = “variation explained by the predictor x” \\(\\boxed{df_R = p}\\) (p = number of predictors, p=1 for SLR) \\(\\boxed{SSE = \\sum_{i=1}^n (\\hat{y_i} - y_i)^2}\\) = error sum of squares = “variation not explained by the predictor x” \\(\\boxed{df_E = n-p-1}\\) Note: We can estimate \\(\\sigma^2\\) with \\(s^2 = MSE\\). Example 11.6 Here’s how to get the ANOVA output for the car stopping example. anova(model) ## Analysis of Variance Table ## ## Response: dist ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## speed 1 21186 21185.5 89.567 1.49e-12 *** ## Residuals 48 11354 236.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Coefficient of Determination, \\(R^2\\) \\(\\boxed{R^2 = \\frac{SSR}{SST}}\\) This is the proportion of the variation in Y that is explained by X. Higher \\(R^2\\) is typically preferred and suggests a better “fit” (careful!) \\(R^2 = r_{xy}^2\\), where \\(r_{xy}\\) is the correlation coefficient. Linear Regression Hypothesis Test Is there a linear relationship between the predictor(s) and the response? \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\neq 0\\) \\(\\boxed{F_{test} = \\frac{MSR}{MSE}}\\) (\\(df_1 = p\\), \\(df_2 = n-p-1\\)) For SLR, this is equivalent to a t-test. If \\(F_{test}\\) is large, then there is evidence that there is a significant linear relationship between the predictor(s) and the response. Example 11.7 Let’s look again at regression output for the cars dataset to see the results of the hypothesis test. summary(model) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 Inferences About the Slope Parameter \\(\\beta_1\\) Assuming we have met the assumptions of linear regression (i.e., IID normal errors), then we can create a confidence interval for \\(\\beta_1\\) or run a hypothesis test of linear relationship (\\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\neq 0\\). \\(E[b_1] = \\beta_1\\) (\\(b_1\\) is an unbiased estimator of \\(\\beta_1\\)) \\(Var(b_1) = \\sigma_{b_1}^2 = \\frac{\\sigma^2}{S_{xx}} = \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}\\) \\(s_{b_1} = \\frac{s}{\\sqrt{S_{xx}}} = \\frac{s}{\\sqrt{\\sum (x_i - \\bar{x})^2}}\\) The estimator \\(b_1\\) has a normal distribution. \\(t = \\frac{b_1 - \\beta_1}{s_{b_1}}\\) has a t-distribution with \\(n-p-1\\) degrees of freedom. Example 11.8 Create a 95% confidence interval for \\(\\beta_1\\) using the “cars” dataset and determine if there is evidence of a linear relationship between and . confint(model) ## 2.5 % 97.5 % ## (Intercept) -31.167850 -3.990340 ## speed 3.096964 4.767853 11.5 The Prediction of Future Y Values Given a specific value of X (denoted \\(x_p\\)), we can use the estimated regression equation to calculate a point estimate or fitted value of Y, denoted \\(\\hat{y}_p\\), by simply plugging \\(x_p\\) into the estimated regression equation. \\(\\hat{y}_p = b_0 + b_1 x_p\\) The residual of the i\\(^{th}\\) observation is \\(r_i = y_i - \\hat{y}_i = \\text{actual y value} - \\text{predicted y value}\\). Example 11.9 Suppose we want to estimate the stopping distance for a car going 20.5mph. We can get this estimate by simply plugging this value into our estimated regression equation. Recall that the estimated regression equation is: \\(\\hat{y} = -17.5791 + 3.9324 x\\). In the above example, we computed a point estimate for the stopping distance of a car going 20.5mph. As with other topics we’ve explored so far in statistics, we prefer to have a measure of uncertainty attached with all point estimates. For regression, this leads us to two kinds of intervals: Confidence interval for mean response Prediction interval for a new data value Confidence intervals for mean response tell us how well we have determined the mean function, \\(E[Y|X]\\), i.e., the line of best fit. Prediction intervals for a new data value tell us how well we can estimate the dependent value for a given new predictor value. These are fundamentally two different kinds of intervals. Confidence Interval for Mean Response A confidence interval for mean response creates an uncertainty estimate for the line of best fit, i.e., the mean function, \\(E[Y|X]\\). In other words, a confidence interval for mean response creates an interval of plausible values for the average response value for a given predictor value. Theorem 11.1 Let \\(x_p\\) be the value of the predictor variable and let \\(\\hat{y}_p = b_0 + b_1 x_p\\) be the fitted value. Then: \\(E[\\hat{y}_p] = \\beta_0 + \\beta_1 x_p\\) \\(Var(\\hat{y}_p) = MSE \\cdot \\left(\\frac{1}{n} + \\frac{(x_p - \\bar{x})^2}{\\sum (x_i -\\bar{x})^2}\\right)\\) \\(SE(\\hat{y}_p) = \\sqrt{MSE \\cdot \\left(\\frac{1}{n} + \\frac{(x_p - \\bar{x})^2}{\\sum (x_i -\\bar{x})^2}\\right)}\\) A confidence interval for the mean response is given by: \\[\\hat{y}_p \\pm t_{\\alpha/2,n-2} \\cdot \\sqrt{MSE \\cdot \\left(\\frac{1}{n} + \\frac{(x_p - \\bar{x})^2}{\\sum (x_i -\\bar{x})^2}\\right)} \\] The theorem above allows for us to create confidence intervals for \\(E[y_p]\\), i.e., the mean response for a given x-value. Example 11.10 Create a confidence interval for the mean response stopping distance for a car moving at 20.5 miles per hour. Note: \\(n=50\\), \\(\\bar{x} = 15.4\\), \\(\\sum (x_i-\\bar{x})^2 = 1370\\), \\(\\sqrt{MSE} = 15.38\\), \\(t_{c.v.}=2.01\\). # t critical value (95% CI, df=48) qt(.975,48) ## [1] 2.010635 # create confidence interval for mean response at x_p = 20.5 predict(model, newdata = data.frame(speed=20.5), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 63.03528 56.92968 69.14089 We can also create a simultaneous confidence band for mean response for the entire regression line. Note that these band are parabolic. Example 11.11 Create a 95% confidence band for the mean response of the cars dataset. # plot the data and the line of best fit plot(cars) abline(model) # create a grid of speed values and put into a data frame speed.grid = seq(min(cars$speed), max(cars$speed), by=0.1) speed.df = data.frame(speed=speed.grid) # calculate the confidence interval at each grid point and plot conf_interval = predict(model, newdata=speed.df, interval=&quot;confidence&quot;,level = 0.95) lines(speed.grid, conf_interval[,2], col=&quot;blue&quot;, lty=2) lines(speed.grid, conf_interval[,3], col=&quot;blue&quot;, lty=2) Is it plausible that the mean response at \\(x_p=20\\) mph is 80 feet? Prediction Interval for a New Data Value A prediction interval for a new data value creates an uncertainty estimate for a new data value. Note that this uncertainty is a combination of uncertainty from the line of best fit as well as the uncertainty from a single new observation. In other words, a prediction interval for a new data value creates an interval of plausible values for the response value for a given new predictor value. Theorem 11.2 Let \\(x_p\\) be the value of the predictor variable and let \\(\\hat{y}_p = b_0 + b_1 x_p\\) be the fitted value. Then: \\(E[\\hat{y}_p] = \\beta_0 + \\beta_1 x_p\\) \\(Var(\\hat{y}_p) = MSE \\cdot \\left(1 + \\frac{1}{n} + \\frac{(x_p - \\bar{x})^2}{\\sum (x_i -\\bar{x})^2}\\right)\\) \\(SE(\\hat{y}_p) = \\sqrt{MSE \\cdot \\left(1 +\\frac{1}{n} + \\frac{(x_p - \\bar{x})^2}{\\sum (x_i -\\bar{x})^2}\\right)}\\) A prediction interval for a new observation is given by: \\[\\hat{y}_p \\pm t_{\\alpha/2,n-2} \\cdot \\sqrt{MSE \\cdot \\left(1 + \\frac{1}{n} + \\frac{(x_p - \\bar{x})^2}{\\sum (x_i -\\bar{x})^2}\\right)} \\] The theorem above allows for us to create prediction intervals for \\(y_p\\), i.e., the response for a new x-value..\\ Example 11.12 Create a 95% prediction interval for a car moving at 20.5 miles per hour. Note: \\(n=50\\), \\(\\bar{x} = 15.4\\), \\(\\sum (x_i-\\bar{x})^2 = 1370\\), \\(\\sqrt{MSE} = 15.38\\), \\(t_{c.v.}=2.01\\). # create confidence interval for mean response at x_p = 20.5 predict(model, newdata = data.frame(speed=20.5), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 63.03528 31.51555 94.55502 We can also create a simultaneous prediction band for a new data value for the entire regression line. Note that these band are linear and parallel. Example 11.13 Create a 95% prediction band for a new response of the cars dataset. # plot the data and the line of best fit plot(cars) abline(model) # create a grid of speed values and put into a data frame speed.grid = seq(min(cars$speed), max(cars$speed), by=0.1) speed.df = data.frame(speed=speed.grid) # calculate the confidence interval at each grid point and plot conf_interval = predict(model, newdata=speed.df, interval=&quot;prediction&quot;,level = 0.95) lines(speed.grid, conf_interval[,2], col=&quot;blue&quot;, lty=2) lines(speed.grid, conf_interval[,3], col=&quot;blue&quot;, lty=2) Is it plausible that a car going 20 mph has a stopping distance of 80 feet? 11.6 Model Diagnostics Recall: Simple linear regression makes a number of assumptions which should be checked to make sure that such a model is appropriate. Assumptions: Linearity: the mean response of the response variable is a linear function of the predictor variable. In multiple regression, the response variable is a linear combination of the predictor variables. Constant Variance: different values of the predictor variables have the same variance in their errors. This is also called homoscedasticity. Normal Errors: all errors are normally distributed (with constant variance). Independent Errors: all errors are independent. Examples of how to diagnose assumptions (1)-(3) will be given with toy datasets that either meet or violate the assumptions. Here’s how to obtain diagnostic plots in R. model = lm(dist~speed,data=cars) plot(model) Linearity Two toy datasets will be created, one linear and one nonlinear. Residual analysis will be used identify violations to the linearity assumption. set.seed(2020) n=100 x = runif(n) # some random x-values y1 = x + rnorm(n,sd=0.1) # linear y2 = x^2 + rnorm(n,sd=0.1) # nonlinear Constant Variance Two toy datasets will be created, one with constant variance and one with nonconstant. Residual analysis will be used identify violations to the constant variance assumption. set.seed(2020) n=100 x = runif(n) # some random x-values y1 = x + rnorm(n,sd=1) # constant variance y2 = x + rnorm(n,sd=x) # nonconstant variance Normal Errors Two toy datasets will be created, one with normal errors and one with Cauchy errors. A QQ-plot will be used identify violations to the normal errors assumption. set.seed(2020) n=100 x = runif(n) # some random x-values y1 = x + rnorm(n,sd=0.1) # normal errors y2 = x + rcauchy(n,scale = 0.01) # laplace errors 11.7 Extrapolation Extrapolation Extrapolation involves using a regression model to predict values outside the observed data range. Such predictions can be unreliable, as relationships may not hold beyond the data limits, and unknown factors can affect outcomes. Caution is advised when extrapolating beyond the observed \\(X\\)-values. Here are two examples: Estimating Housing Prices: A model based on recent trends might suggest a steady increase in housing prices. However, extrapolating beyond the observed data range could overlook economic shifts or policy changes, resulting in overly optimistic predictions. Drug Dosage Effects: In medical studies, the relationship between dosage and effectiveness may appear linear within a safe range. Extrapolating to higher doses could ignore toxicity limits, leading to predictions that overestimate benefits and underestimate potential risks. Example 11.14 The age and weight of Aaron’s cat, Ellie, is plotted below for her first 10 months. How much would you predict Ellie weighed after 1000 days? [Caution: Extrapolation!] Below is Ellie’s weight up until day 1000. How does this compare with prediction using data up until day 300? 11.8 R Companion for Chapter 11 Example 11.15 The dataset is provided in the base version of R. The data is on the relation between temperature in degrees Celsius and vapor pressure of mercury in millimeters (of mercury). Let’s begin by looking at the header of the data and calculating the correlation. data(pressure) head(pressure) ## temperature pressure ## 1 0 0.0002 ## 2 20 0.0012 ## 3 40 0.0060 ## 4 60 0.0300 ## 5 80 0.0900 ## 6 100 0.2700 cor(pressure) ## temperature pressure ## temperature 1.0000000 0.7577923 ## pressure 0.7577923 1.0000000 The correlation matrix is given in the example above. The diagonal elements of the correlation matrix shows that any variable is perfectly correlated with itself (i.e., \\(r_{xx} = 1\\)). In the off diagonal, you can see the correlation between the and is 0.758 which a strong, positive linear relationship. Let’s fit a simple linear regression model with the being the predictor variable and being the response variable. model = lm(pressure~temperature,data=pressure) summary(model) ## ## Call: ## lm(formula = pressure ~ temperature, data = pressure) ## ## Residuals: ## Min 1Q Median 3Q Max ## -158.08 -117.06 -32.84 72.30 409.43 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -147.8989 66.5529 -2.222 0.040124 * ## temperature 1.5124 0.3158 4.788 0.000171 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 150.8 on 17 degrees of freedom ## Multiple R-squared: 0.5742, Adjusted R-squared: 0.5492 ## F-statistic: 22.93 on 1 and 17 DF, p-value: 0.000171 The estimated regression is \\(\\hat{pressure} = -147.90 + 1.51 \\cdot temperature\\). A one degree Celsius increase in temperate of mercury is associated with an increase of 1.51 in vapor pressure.\\ Is this model appropriate? Let’s plot the data along with the line of best fit. plot(pressure~temperature,data=pressure) abline(model) As seen in the above plot, our simple linear regression model is completely inadequate. We could determine that a nonlinearity exists by looking at the first residual diagnostic plot. plot(model,which=1) In this diagnostic plot, we see strong evidence of an unaccounted for nonlinearity as the shape of the graph is not approximately flat. Based on the scatterplot on the previous page, there appears to be a possibly exponential relationship between temperature and pressure. Let’s try log transforming pressure and refitting the linear regression model. (Note: Variable transformations are covered in Chapter 12.) model2 = lm(log(pressure)~temperature,data=pressure) summary(model2) ## ## Call: ## lm(formula = log(pressure) ~ temperature, data = pressure) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4491 -0.6876 0.2866 0.8716 1.1365 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.068144 0.483831 -12.54 5.10e-10 *** ## temperature 0.039792 0.002296 17.33 3.07e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.096 on 17 degrees of freedom ## Multiple R-squared: 0.9464, Adjusted R-squared: 0.9433 ## F-statistic: 300.3 on 1 and 17 DF, p-value: 3.07e-12 plot(pressure~temperature,data=pressure) x.new = 0:350 y.new = predict(model2,newdata=list(temperature=x.new,interval=&quot;confidence&quot;)) lines(x.new,exp(y.new)) This model seems to fit the data better than the untransformed simple linear regression model but is still somewhat inadequate for temperatures greater than 300C. Additionally, the residual diagnostic plot suggest our model is inadequate. A better model should be found if modeling temperatures above 300C is of interest. plot(model2,which=1) "],["multiple-regression.html", "Chapter 12 Multiple Regression 12.1 Multiple Regression Model 12.2 Variable Transformations 12.3 Categorical Variables, Interaction, and Polynomial Regression 12.4 Polynomial regression 12.5 Model and Variable Selection 12.6 Other Considerations 12.7 R Companion for Chapter 12", " Chapter 12 Multiple Regression In Chapter 11, we explored simple linear regression to predict a quantitative variable, \\(y\\), using a single predictor, \\(x\\). Multiple regression expands this approach by incorporating two or more predictor variables, enabling more comprehensive models. This chapter introduces techniques like variable transformations to improve linearity, interaction terms to capture relationships between predictors, and polynomial regression for modeling curved trends. We also explore model selection strategies to identify the most effective combination of predictors for accurate, reliable predictions. 12.1 Multiple Regression Model Multiple regression (MR) extends simple linear regression by allowing for multiple predictor variables to model a single response variable. Population Models for SLR and MR: SLR: \\(y = \\beta_0 + \\beta_1 x_1 + \\epsilon\\) MR: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon\\) Estimated Regression Equations for SLR and MR: SLR: \\(\\hat{y} = b_0 + b_1 x_1\\) MR: \\(\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\ldots + b_p x_p\\) Assumptions for Multiple Regression Errors (Independence and Normality): The errors (\\(\\epsilon_i\\)) should be independent and identically distributed normal random variables with a mean of 0 and a constant variance (\\(\\sigma^2\\)). This assumption ensures that the residuals are not systematically related to each other, which can lead to biased estimates. Linearity: The mean of the response variable is assumed to be a linear combination of the predictor variables. This means the relationship between the predictors and the response should be approximately linear. Residual plots can help identify whether this assumption holds. Homoscedasticity (Constant Variance of Errors): The errors should have constant variance across all levels of the predictor variables. If the variance of the residuals increases or decreases with the value of the predictors (i.e., heteroscedasticity), it indicates potential issues. A residual plot can help detect heteroscedasticity—look for a “fanning out” or “fanning in” pattern, which suggests non-constant variance. Note: While estimated regression coefficients can be calculated using matrix methods from linear algebra, we will focus on using R for these calculations. Example 12.1 The “trees” dataset in R provides measurements of the girth, height, and volume of 31 timber in 31 felled black cherry trees. We can fit a basic linear regression model with height and girth as predictor variables in R as follows. data(trees) mod = lm(Volume~Height+Girth,data=trees) summary(mod) ## ## Call: ## lm(formula = Volume ~ Height + Girth, data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 F-test for Overall Model (Model Utility Test) We want to test if any of our predictor variables (\\(x_i&#39;s\\)) are linearly related to the response variable (y). If we have \\(p\\) predictor variables, then our hypotheses will be: \\(\\boxed{F_{test} = \\frac{MSR}{MSE}, df_1 = p, df_2 = n-p-1}\\) This will only tell us if any of our predictors are linearly related to the response, not which ones. Example 12.2 Using the output from the previous page, complete an F-test for overall model for the cherry trees dataset. t-test for Individual Predictors If we reject \\(H_o\\) in our F-test, then we conclude that one or more of our predictor variables is linearly related to our response variable. We can complete t-tests for each predictor to determine which are useful/significant. For example, we can test an individual predictor with the following hypothesis test: \\(\\boxed{t_{test} = \\frac{b_i}{s_{b_i}}, df = n-p-1}\\) Rule of Thumb: If \\(\\beta_i\\) is not significantly different from zero (p-value \\(\\geq \\alpha\\)), consider dropping that predictor from the model. However, evaluate practical significance and model context before removing variables entirely. Example 12.3 Using the output from the previous page, complete t-tests for individual predictors (height and diameter). Interpreting the model estimates (\\(b_i\\)) Example 12.4 Interpret \\(b_2\\) from the cherry trees example in the context of the problem. 12.2 Variable Transformations Sometimes a nonlinear regression function is appropriate which may require us to transform the data. An example of such a nonlinearity is given below. Transforming Response Variable Example 12.5 A colony of bacteria is exposed to X-rays and the number of surviving bacteria, \\(n\\), at time \\(t\\) are recorded. Let’s examine the data, the SLR line of best fit, and a residual plot. \\(t\\) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\(n_t\\) 355 211 197 166 142 166 104 60 56 38 36 32 21 19 15 By inspection, the line of best fit does not seem appropriate. Additionally, the residual plot suggests that the data has a nonlinearity. Instead, we could consider the following model: \\(n_t = n_0 \\cdot e^{\\beta t}\\). If we take the natural log of each side of this model, we get: \\(ln(n_t) = ln(n_0) + ln(e^{\\beta t}) = \\alpha + \\beta t\\). \\(t\\) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\(ln(n_t)\\) 5.87 5.35 5.28 5.11 4.96 5.11 4.64 4.09 4.03 3.64 3.58 3.47 3.04 2.94 2.71 After making this transformation, we can refit the regression model with a much more appropriate model. A regression model is considered intrinsically linear if, through a transformation on \\(Y\\) or \\(X_i\\), it can be expressed as a linear regression model of the form \\(y&#39; = \\beta_0 + \\beta_1 x&#39; + \\epsilon&#39;\\). Examples: \\(y = \\alpha e^{\\beta x} \\cdot \\epsilon\\) transforms to \\(\\ln(y) = y&#39; = \\beta_0 + \\beta_1 x&#39; + \\epsilon&#39;\\), where \\(x&#39; = x\\), \\(\\beta_0 = \\ln(\\alpha)\\), \\(\\beta_1 = \\beta\\), and \\(\\epsilon&#39; = \\ln(\\epsilon)\\). \\(y = \\alpha x^\\beta \\cdot \\epsilon\\) transforms to \\(\\log(y) = y&#39; = \\beta_0 + \\beta_1 x&#39; + \\epsilon&#39;\\), where \\(x&#39; = \\log(x)\\), \\(\\beta_0 = \\log(\\alpha)\\), \\(\\beta_1 = \\beta\\), and \\(\\epsilon&#39; = \\log(\\epsilon)\\). Box-Cox Transformation If the linearity or constant variance assumptions are violated, consider using a Box-Cox transformation to find an appropriate transformation for the response variable. This method can help determine the best power transformation to improve the model fit, making it more linear and stabilizing variance. The boxcox() function in R can be used to explore possible transformations. library(MASS) boxcox(n~t) Transforming Predictor Variables We can also transform our predictor variables, \\(X_i\\), if a nonlinear model is appropriate. Consider the following example with a quadratic relationship. Example 12.6 From Newtonian Physics, acceleration of an object in free fall drops a distance that is proportional to the square of the elapsed time. Consider the following dataset in which an object’s dropped distance as a function of time and some measurement error is present. \\(t\\) 0 1 2 3 4 5 6 7 8 9 10 \\(d_t\\) 0 8.7 20.6 37.1 82.2 120.9 177.5 244.2 310.1 399.0 480.6 First, we will fit the model \\(d = b_0 + b_1 t\\) and then compare this model with a quadratic model, \\(d = b_0 + b_1 t^2\\). t = 0:10 d = c(0,8.7,20.6,37.1,82.2,120.9,177.5,244.2,310.1,399.0,480.6) lm1 = lm(d~t) lm2 = lm(d~I(t^2)) By inspection, the quadratic model seems more appropriate in terms of the line of best fit and in terms of the residuals. 12.3 Categorical Variables, Interaction, and Polynomial Regression Categorical Variables Categorical variables can be easily incorporated in a regression model. In fact, ANOVA from chapter 10 can be thought of as a regression on a categorical variable. Example 12.7 The following dataset in R was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). data(mtcars) head(mtcars,n=4) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 To include a categorical variable in a regression model, you need to designate it as a “factor” variable in R. Let’s fit a multiple regression model with miles per gallon as the response variable, weight as a continuous predictor, and the number of cylinders (4, 6, or 8) as a categorical predictor. mt.lm = lm(mpg~wt+factor(cyl),data=mtcars) summary(mt.lm) ## ## Call: ## lm(formula = mpg ~ wt + factor(cyl), data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5890 -1.2357 -0.5159 1.3845 5.7915 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.9908 1.8878 18.006 &lt; 2e-16 *** ## wt -3.2056 0.7539 -4.252 0.000213 *** ## factor(cyl)6 -4.2556 1.3861 -3.070 0.004718 ** ## factor(cyl)8 -6.0709 1.6523 -3.674 0.000999 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.557 on 28 degrees of freedom ## Multiple R-squared: 0.8374, Adjusted R-squared: 0.82 ## F-statistic: 48.08 on 3 and 28 DF, p-value: 3.594e-11 Example 12.8 For the Motor Trends car example, answer the following questions. Write the estimated multiple regression equation. What is the estimated MPG for a car that weighs 2.900 (thousands of pounds) and has 4 cylinders. What is the estimated difference in MPG between an 8-cylinder car and a 6-cylinder car? Interpret the estimated regression coefficients in the context of the problem. Interaction If the effect of one predictor variable on the response variable depends on other predictor variables, we may want to include an interaction variable. Example 12.9 Let’s fit the following model: \\(MPG = b_0 + b_1 \\cdot Weight + b_2 \\cdot I(am) + b_3 \\cdot Weight \\cdot I(am)\\), where the ``am’’ variable is 0 for an automatic transmission and 1 is for a manual transmission. lm.int = lm(mpg~wt*factor(am),data=mtcars) summary(lm.int) ## ## Call: ## lm(formula = mpg ~ wt * factor(am), data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6004 -1.5446 -0.5325 0.9012 6.0909 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.4161 3.0201 10.402 4.00e-11 *** ## wt -3.7859 0.7856 -4.819 4.55e-05 *** ## factor(am)1 14.8784 4.2640 3.489 0.00162 ** ## wt:factor(am)1 -5.2984 1.4447 -3.667 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.591 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8151 ## F-statistic: 46.57 on 3 and 28 DF, p-value: 5.209e-11 12.4 Polynomial regression There is flexibility in a regression model to fit an arbitrary polynomial to the data, however, we must take care to not “overfit” the data. The hierarchical principle in polynomial regression states that if a model includes a higher-order term (e.g., \\(x^2\\)), it should also include all lower-order terms (e.g., \\(x\\)), even if their coefficients are not statistically significant. This ensures the model is mathematically valid and interpretable, as excluding lower-order terms can distort the relationship between the predictor and the response. However, in certain cases, lower-order terms may be omitted if there is a domain-specific justification, such as theoretical knowledge or practical constraints that support the exclusion. Example 12.10 Let’s generate a toy dataset based a cubic polynomial and see how successfully we can recover the original function. ## x y ## [1,] -9.976245 -599.8586 ## [2,] -9.957027 -588.7105 ## [3,] -9.948346 -599.9990 ## [4,] -9.779192 -560.4769 ## [5,] -8.859972 -427.0615 ## [6,] -8.652312 -399.5171 ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.9139 -8.4193 0.2316 6.0311 31.3370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.582774 1.716694 -0.922 0.359 ## x 0.002634 0.516494 0.005 0.996 ## I(x^2) -1.013334 0.039545 -25.625 &lt;2e-16 *** ## I(x^3) 0.497069 0.007792 63.792 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.92 on 96 degrees of freedom ## Multiple R-squared: 0.9961, Adjusted R-squared: 0.996 ## F-statistic: 8212 on 3 and 96 DF, p-value: &lt; 2.2e-16 12.5 Model and Variable Selection Here are some considerations when completing model and variable selection. Statistical Significance — Consider removing predictors with high p-values, as they may not contribute significantly to explaining the variation in the response variable. However, be cautious of multicollinearity, as it can impact p-values. Theoretical and Domain Knowledge — Use knowledge of the field to guide variable selection. Some variables may be essential based on theory, even if their statistical contribution appears marginal. Practical Significance — Beyond statistical tests, evaluate each variable’s real-world relevance. Retain variables that are meaningful in the context of the problem, even if their statistical significance is lower. Multicollinearity — Check for high correlations among predictors, as multicollinearity can inflate standard errors and obscure true relationships. Removing or combining correlated variables can improve model stability. Model Parsimony — Aim for simplicity by retaining only essential variables, as simpler models are often more interpretable and less prone to overfitting. Use metrics like Adjusted \\(R^2\\), AIC, or BIC to balance fit with model complexity. Example 12.11 Returning to the cherry trees example, let’s consider the following regression models for predicting tree volume: Model 1: \\(\\widehat{\\text{volume}} = b_0 + b_1 \\cdot \\text{Height} + b_2 \\cdot \\text{Diameter}\\) Model 2: \\(\\widehat{\\text{volume}} = b_0 + b_1 \\cdot \\text{Height} + b_2 \\cdot \\text{Diameter}^2\\) Model 3: \\(\\widehat{\\text{volume}} = b_0 + b_1 \\cdot \\text{Height} \\cdot \\text{Diameter}^2\\) Model 4: \\(\\widehat{\\text{volume}} = b_1 \\cdot \\text{Height} \\cdot \\text{Diameter}^2\\) Which model is preferred? Model 4 aligns well with our understanding that volume is proportional to \\(\\text{Height} \\times \\text{Diameter}^2\\). But how could we determine the best model if we didn’t already know this relationship? This is where model selection techniques come in, allowing us to evaluate and compare models based on the data alone, without prior knowledge of the underlying formula. lm1 = lm(Volume~Height+Girth,data=trees) summary(lm1) ## ## Call: ## lm(formula = Volume ~ Height + Girth, data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 lm2 = lm(Volume~Height+I(Girth^2),data=trees) summary(lm2) ## ## Call: ## lm(formula = Volume ~ Height + I(Girth^2), data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8844 -2.2105 0.1196 2.6134 4.2404 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -27.511603 6.557697 -4.195 0.000248 *** ## Height 0.348809 0.093152 3.744 0.000830 *** ## I(Girth^2) 0.168458 0.006679 25.222 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.799 on 28 degrees of freedom ## Multiple R-squared: 0.9729, Adjusted R-squared: 0.971 ## F-statistic: 503.2 on 2 and 28 DF, p-value: &lt; 2.2e-16 lm3 = lm(Volume~Height:I(Girth^2),data=trees) summary(lm3) ## ## Call: ## lm(formula = Volume ~ Height:I(Girth^2), data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6195 -1.1002 -0.1656 1.7451 4.1976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.977e-01 9.636e-01 -0.309 0.76 ## Height:I(Girth^2) 2.124e-03 5.949e-05 35.711 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.493 on 29 degrees of freedom ## Multiple R-squared: 0.9778, Adjusted R-squared: 0.977 ## F-statistic: 1275 on 1 and 29 DF, p-value: &lt; 2.2e-16 lm4 = lm(Volume~Height:I(Girth^2)-1,data=trees) summary(lm4) ## ## Call: ## lm(formula = Volume ~ Height:I(Girth^2) - 1, data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6696 -1.0832 -0.3341 1.6045 4.2944 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Height:I(Girth^2) 2.108e-03 2.722e-05 77.44 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.455 on 30 degrees of freedom ## Multiple R-squared: 0.995, Adjusted R-squared: 0.9949 ## F-statistic: 5996 on 1 and 30 DF, p-value: &lt; 2.2e-16 Model Residuals How can model residuals help us pick the appropriate model? 12.6 Other Considerations 1. Multicollinearity — When two or more predictor variables are highly correlated, it can lead to large standard errors, making it difficult to assess the individual significance of each predictor. This may result in predictors appearing insignificant, even if they are relevant to the model. 2. Dependent Errors — If the errors (residuals) are correlated, the model may be inadequate, as this violates the assumption of independence in regression. In such cases, a time series model may be more appropriate. For example, stock prices often exhibit high autocorrelation (i.e., errors are highly correlated), which indicates the need for specialized models designed to account for time-based dependencies. 3. Confounding Variables — Excluding important variables from a model can lead to incorrect inferences, as confounding variables may influence both the predictor and response variables. For instance, in the example of shark attacks and ice cream sales, there appeared to be a strong relationship between the two. However, the true underlying factor was the temperature: warmer days increase both swimming (leading to more shark interactions) and ice cream sales. Failing to account for such confounding variables can lead to misleading conclusions. 4. Outliers and Influential Points — Outliers or points with high leverage can disproportionately impact the model, skewing results or making the model appear significant when it isn’t. Identifying and investigating outliers can help determine whether to keep them, transform the data, or adjust the model. 5. Overfitting — Adding too many predictors can lead to overfitting, where the model performs well on the sample data but poorly on new data. Techniques like cross-validation, adjusted \\(R^2\\), and criteria such as AIC or BIC can help assess the model’s generalizability and prevent overfitting. 6. Data Scaling and Centering — Predictor variables on vastly different scales can lead to numerical instability and difficulty interpreting coefficients, especially with interaction terms. Scaling or centering predictors can help stabilize the model and improve interpretability. 12.7 R Companion for Chapter 12 Example 12.12 The dataset is provide in the base version of R and is a macroeconomic data set which provides a well-known example for a highly collinear regression. This dataset includes 7 economical variables, observed yearly from 1947 to 1962 (n=16). For more information on the variables, enter into R. Let’s begin by looking at the header of the dataset and calculating the correlation matrix. &lt;&lt;setup2, include=FALSE, cache=FALSE, tidy=TRUE&gt;&gt;= options(tidy=TRUE, width=90) @ data(longley) # show first few observations head(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 ## 1950 89.5 284.599 335.1 165.0 110.929 1950 61.187 ## 1951 96.2 328.975 209.9 309.9 112.075 1951 63.221 ## 1952 98.1 346.999 193.2 359.4 113.270 1952 63.639 # round the correlations to two decimal places for better viewing round(cor(longley),2) ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## GNP.deflator 1.00 0.99 0.62 0.46 0.98 0.99 0.97 ## GNP 0.99 1.00 0.60 0.45 0.99 1.00 0.98 ## Unemployed 0.62 0.60 1.00 -0.18 0.69 0.67 0.50 ## Armed.Forces 0.46 0.45 -0.18 1.00 0.36 0.42 0.46 ## Population 0.98 0.99 0.69 0.36 1.00 0.99 0.96 ## Year 0.99 1.00 0.67 0.42 0.99 1.00 0.97 ## Employed 0.97 0.98 0.50 0.46 0.96 0.97 1.00 options(tidy=TRUE, width=70) For this example, we will use (number of people employed) as the response variable and the other six variables as predictor variables. Let’s begin by using all six predictor variables and fitting the multiple regression model. model1 = lm(Employed~GNP.deflator+GNP+Unemployed+Armed.Forces+Population+Year, data=longley) summary(model1) ## ## Call: ## lm(formula = Employed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + ## Population + Year, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.41011 -0.15767 -0.02816 0.10155 0.45539 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.482e+03 8.904e+02 -3.911 0.003560 ** ## GNP.deflator 1.506e-02 8.492e-02 0.177 0.863141 ## GNP -3.582e-02 3.349e-02 -1.070 0.312681 ## Unemployed -2.020e-02 4.884e-03 -4.136 0.002535 ** ## Armed.Forces -1.033e-02 2.143e-03 -4.822 0.000944 *** ## Population -5.110e-02 2.261e-01 -0.226 0.826212 ## Year 1.829e+00 4.555e-01 4.016 0.003037 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3049 on 9 degrees of freedom ## Multiple R-squared: 0.9955, Adjusted R-squared: 0.9925 ## F-statistic: 330.3 on 6 and 9 DF, p-value: 4.984e-10 As can be seen in the above output, three variables (, , and ) are not significant. Let’s take a stepwise approach, remove the variable with the largest p-value that is not statistically significant, and refit the model. In this case, let’s remove the predictor variable and refit the multiple regression model. model2 = lm(Employed~GNP+Unemployed+Armed.Forces+Population+Year, data=longley) summary(model2) ## ## Call: ## lm(formula = Employed ~ GNP + Unemployed + Armed.Forces + Population + ## Year, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43015 -0.15399 -0.01832 0.10081 0.44964 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.450e+03 8.282e+02 -4.165 0.001932 ** ## GNP -3.196e-02 2.420e-02 -1.321 0.216073 ## Unemployed -1.972e-02 3.861e-03 -5.108 0.000459 *** ## Armed.Forces -1.020e-02 1.908e-03 -5.345 0.000326 *** ## Population -7.754e-02 1.616e-01 -0.480 0.641607 ## Year 1.814e+00 4.253e-01 4.266 0.001648 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2897 on 10 degrees of freedom ## Multiple R-squared: 0.9955, Adjusted R-squared: 0.9932 ## F-statistic: 438.8 on 5 and 10 DF, p-value: 2.242e-11 The variable is again statistically not significant and has the largest p-value, so let’s remove that from the model. model3 = lm(Employed~GNP+Unemployed+Armed.Forces+Year, data=longley) summary(model3) ## ## Call: ## lm(formula = Employed ~ GNP + Unemployed + Armed.Forces + Year, ## data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.42165 -0.12457 -0.02416 0.08369 0.45268 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.599e+03 7.406e+02 -4.859 0.000503 *** ## GNP -4.019e-02 1.647e-02 -2.440 0.032833 * ## Unemployed -2.088e-02 2.900e-03 -7.202 1.75e-05 *** ## Armed.Forces -1.015e-02 1.837e-03 -5.522 0.000180 *** ## Year 1.887e+00 3.828e-01 4.931 0.000449 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2794 on 11 degrees of freedom ## Multiple R-squared: 0.9954, Adjusted R-squared: 0.9937 ## F-statistic: 589.8 on 4 and 11 DF, p-value: 9.5e-13 All of the predictor variables are now statistically significant at \\(\\alpha=0.05\\). Note that \\(R^2 = 0.9954\\), so 99.54% of the variability in the number of employed people can be explained by the linear relationship with gross national product (GNP), number of unemployed people, number of people in the armed forces, and year. This is a very large value for \\(R^2\\) which suggests a very good model fit. One caution is that our predictor variables are highly correlated (see correlation matrix at the beginning of the example), so this could result in high standard errors. This concern of is beyond the scope of this class and is covered in more advanced statistics courses. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
